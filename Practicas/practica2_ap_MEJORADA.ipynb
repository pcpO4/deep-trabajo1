{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZmnfT5gAXaJ"
   },
   "source": [
    "# Pr√°ctica 2 MEJORADA: Estudio detallado de hiperpar√°metros\n",
    "\n",
    "**Objetivo**: Estudiar el comportamiento de los distintos par√°metros de entrenamiento y crear un modelo optimizado para clasificar n√∫meros pares vs impares.\n",
    "\n",
    "## Par√°metros a estudiar:\n",
    "- N√∫mero de neuronas en la capa oculta\n",
    "- N√∫mero de √©pocas\n",
    "- Funci√≥n de p√©rdida (loss)\n",
    "- Tama√±o de lote (batch size)\n",
    "- Tasa de aprendizaje (learning rate)\n",
    "- Porcentaje de validaci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSLL2NXq6Ksz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Configuraci√≥n para reproducibilidad\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cargar datos MNIST y crear etiquetas par/impar\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convertir etiquetas originales (0-9) a par/impar (0=par, 1=impar)\n",
    "y_train_parimpar = y_train % 2\n",
    "y_test_parimpar = y_test % 2\n",
    "\n",
    "print(f\"Datos de entrenamiento: {x_train.shape}\")\n",
    "print(f\"Datos de test: {x_test.shape}\")\n",
    "print(f\"Distribuci√≥n par/impar en entrenamiento: {np.bincount(y_train_parimpar)}\")\n",
    "print(f\"Distribuci√≥n par/impar en test: {np.bincount(y_test_parimpar)}\")\n",
    "\n",
    "# Tambi√©n mantenemos las etiquetas originales para comparaci√≥n\n",
    "print(f\"Distribuci√≥n original en entrenamiento: {np.bincount(y_train)}\")\n",
    "print(f\"Distribuci√≥n original en test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Estudio sistem√°tico de hiperpar√°metros\n",
    "\n",
    "Vamos a realizar un estudio exhaustivo de cada hiperpar√°metro por separado para entender su impacto individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_modelo(n_neuronas=128, learning_rate=0.001, optimizer='adam'):\n",
    "    \"\"\"Funci√≥n auxiliar para crear modelo consistente\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(n_neuronas, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Para clasificaci√≥n 0-9\n",
    "    ])\n",
    "    \n",
    "    # Configurar optimizador\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    model.compile(optimizer=opt, \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def crear_modelo_parimpar(n_neuronas=128, learning_rate=0.001, dropout=0.0):\n",
    "    \"\"\"Modelo espec√≠fico para clasificaci√≥n par/impar\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(n_neuronas, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Capa adicional\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Clasificaci√≥n binaria\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluar_modelo(model, x_test, y_test, nombre_experimento, tiempo_entrenamiento):\n",
    "    \"\"\"Funci√≥n para evaluar modelo y retornar m√©tricas\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(x_test, verbose=0)\n",
    "    \n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] == 1:\n",
    "        # Clasificaci√≥n binaria\n",
    "        y_pred_class = (y_pred > 0.5).astype(int).flatten()\n",
    "    else:\n",
    "        # Clasificaci√≥n multiclase\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'experimento': nombre_experimento,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'tiempo_entrenamiento': tiempo_entrenamiento,\n",
    "        'predicciones': y_pred_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Estudio del n√∫mero de neuronas en la capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: N√∫mero de neuronas\n",
    "neuronas_valores = [32, 64, 128, 256, 512]\n",
    "resultados_neuronas = []\n",
    "\n",
    "print(\"=== ESTUDIO: N√öMERO DE NEURONAS ===\")\n",
    "\n",
    "for n_neuronas in neuronas_valores:\n",
    "    print(f\"\\nEvaluando {n_neuronas} neuronas...\")\n",
    "    \n",
    "    model = crear_modelo(n_neuronas=n_neuronas)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train, \n",
    "                       epochs=10, \n",
    "                       batch_size=128,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = evaluar_modelo(model, x_test, y_test, \n",
    "                              f\"{n_neuronas} neuronas\", \n",
    "                              tiempo_entrenamiento)\n",
    "    resultado['n_neuronas'] = n_neuronas\n",
    "    resultado['val_accuracy'] = history.history['val_accuracy'][-1]\n",
    "    resultado['history'] = history\n",
    "    resultados_neuronas.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {resultado['test_accuracy']:.4f}\")\n",
    "    print(f\"  Val Accuracy: {resultado['val_accuracy']:.4f}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n resultados neuronas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Impacto del N√∫mero de Neuronas', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy vs Neuronas\n",
    "neuronas = [r['n_neuronas'] for r in resultados_neuronas]\n",
    "test_accs = [r['test_accuracy'] for r in resultados_neuronas]\n",
    "val_accs = [r['val_accuracy'] for r in resultados_neuronas]\n",
    "\n",
    "axes[0,0].plot(neuronas, test_accs, 'o-', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "axes[0,0].plot(neuronas, val_accs, 's-', label='Validation Accuracy', linewidth=2, markersize=8)\n",
    "axes[0,0].set_xlabel('N√∫mero de Neuronas')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_title('Accuracy vs N√∫mero de Neuronas')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "tiempos = [r['tiempo_entrenamiento'] for r in resultados_neuronas]\n",
    "axes[0,1].bar(neuronas, tiempos, color='coral', alpha=0.7)\n",
    "axes[0,1].set_xlabel('N√∫mero de Neuronas')\n",
    "axes[0,1].set_ylabel('Tiempo (s)')\n",
    "axes[0,1].set_title('Tiempo de Entrenamiento vs Neuronas')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Curvas de aprendizaje para el mejor modelo\n",
    "mejor_modelo = max(resultados_neuronas, key=lambda x: x['test_accuracy'])\n",
    "history = mejor_modelo['history']\n",
    "\n",
    "epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "axes[1,0].plot(epochs_range, history.history['accuracy'], 'b-', label='Train Accuracy')\n",
    "axes[1,0].plot(epochs_range, history.history['val_accuracy'], 'r--', label='Val Accuracy')\n",
    "axes[1,0].set_xlabel('√âpocas')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].set_title(f'Mejor Configuraci√≥n: {mejor_modelo[\"n_neuronas\"]} neuronas')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# An√°lisis overfitting\n",
    "overfitting = [r['history'].history['accuracy'][-1] - r['val_accuracy'] for r in resultados_neuronas]\n",
    "axes[1,1].bar(neuronas, overfitting, color='orange', alpha=0.7)\n",
    "axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1,1].set_xlabel('N√∫mero de Neuronas')\n",
    "axes[1,1].set_ylabel('Train Acc - Val Acc')\n",
    "axes[1,1].set_title('An√°lisis de Overfitting')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen estad√≠stico\n",
    "print(\"\\n=== RESUMEN: IMPACTO DEL N√öMERO DE NEURONAS ===\")\n",
    "for r in resultados_neuronas:\n",
    "    overfitting_val = r['history'].history['accuracy'][-1] - r['val_accuracy']\n",
    "    print(f\"{r['n_neuronas']:3d} neuronas: Test={r['test_accuracy']:.4f}, Val={r['val_accuracy']:.4f}, \", end='')\n",
    "    print(f\"Overfitting={overfitting_val:.4f}, Tiempo={r['tiempo_entrenamiento']:.1f}s\")\n",
    "\n",
    "print(f\"\\nMejor configuraci√≥n: {mejor_modelo['n_neuronas']} neuronas (Accuracy: {mejor_modelo['test_accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Estudio del n√∫mero de √©pocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: N√∫mero de √©pocas\n",
    "epochs_valores = [5, 10, 20, 30, 50]\n",
    "resultados_epochs = []\n",
    "\n",
    "print(\"\\n=== ESTUDIO: N√öMERO DE √âPOCAS ===\")\n",
    "\n",
    "for n_epochs in epochs_valores:\n",
    "    print(f\"\\nEvaluando {n_epochs} √©pocas...\")\n",
    "    \n",
    "    model = crear_modelo(n_neuronas=128)  # Usando mejor valor anterior\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       epochs=n_epochs,\n",
    "                       batch_size=128,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = evaluar_modelo(model, x_test, y_test,\n",
    "                              f\"{n_epochs} √©pocas\",\n",
    "                              tiempo_entrenamiento)\n",
    "    resultado['n_epochs'] = n_epochs\n",
    "    resultado['val_accuracy'] = history.history['val_accuracy'][-1]\n",
    "    resultado['history'] = history\n",
    "    resultados_epochs.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {resultado['test_accuracy']:.4f}\")\n",
    "    print(f\"  Val Accuracy: {resultado['val_accuracy']:.4f}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n resultados √©pocas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Impacto del N√∫mero de √âpocas', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Curvas de entrenamiento por √©pocas\n",
    "for resultado in resultados_epochs:\n",
    "    history = resultado['history']\n",
    "    epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "    axes[0,0].plot(epochs_range, history.history['accuracy'], \n",
    "                   label=f\"{resultado['n_epochs']} √©pocas\", alpha=0.8)\n",
    "    axes[0,1].plot(epochs_range, history.history['val_accuracy'], \n",
    "                   '--', label=f\"{resultado['n_epochs']} √©pocas (val)\", alpha=0.8)\n",
    "\n",
    "axes[0,0].set_xlabel('√âpoca')\n",
    "axes[0,0].set_ylabel('Train Accuracy')\n",
    "axes[0,0].set_title('Evoluci√≥n Train Accuracy')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].set_xlabel('√âpoca')\n",
    "axes[0,1].set_ylabel('Validation Accuracy')\n",
    "axes[0,1].set_title('Evoluci√≥n Validation Accuracy')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy final vs √©pocas\n",
    "epochs_vals = [r['n_epochs'] for r in resultados_epochs]\n",
    "test_accs = [r['test_accuracy'] for r in resultados_epochs]\n",
    "val_accs = [r['val_accuracy'] for r in resultados_epochs]\n",
    "\n",
    "axes[1,0].plot(epochs_vals, test_accs, 'o-', label='Test', markersize=8)\n",
    "axes[1,0].plot(epochs_vals, val_accs, 's-', label='Validation', markersize=8)\n",
    "axes[1,0].set_xlabel('N√∫mero de √âpocas')\n",
    "axes[1,0].set_ylabel('Accuracy Final')\n",
    "axes[1,0].set_title('Accuracy Final vs √âpocas')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Eficiencia: Accuracy por segundo\n",
    "tiempos = [r['tiempo_entrenamiento'] for r in resultados_epochs]\n",
    "eficiencia = [acc/tiempo for acc, tiempo in zip(test_accs, tiempos)]\n",
    "\n",
    "axes[1,1].bar(epochs_vals, eficiencia, color='lightgreen', alpha=0.7)\n",
    "axes[1,1].set_xlabel('N√∫mero de √âpocas')\n",
    "axes[1,1].set_ylabel('Accuracy / Segundo')\n",
    "axes[1,1].set_title('Eficiencia de Entrenamiento')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de convergencia\n",
    "print(\"\\n=== AN√ÅLISIS DE CONVERGENCIA ===\")\n",
    "for r in resultados_epochs:\n",
    "    history = r['history']\n",
    "    mejora_final = history.history['val_accuracy'][-1] - history.history['val_accuracy'][4]\n",
    "    print(f\"{r['n_epochs']:2d} √©pocas: Mejora √∫ltimas 5 √©pocas = {mejora_final:.4f}\")\n",
    "\n",
    "mejor_epochs = max(resultados_epochs, key=lambda x: x['test_accuracy'])\n",
    "print(f\"\\nMejor configuraci√≥n: {mejor_epochs['n_epochs']} √©pocas (Accuracy: {mejor_epochs['test_accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Estudio de funciones de p√©rdida (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: Funciones de p√©rdida\n",
    "loss_functions = [\n",
    "    'sparse_categorical_crossentropy',\n",
    "    'categorical_crossentropy'  # Requiere one-hot encoding\n",
    "]\n",
    "\n",
    "resultados_loss = []\n",
    "\n",
    "print(\"\\n=== ESTUDIO: FUNCIONES DE P√âRDIDA ===\")\n",
    "\n",
    "# Convertir etiquetas para categorical_crossentropy\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "for loss_func in loss_functions:\n",
    "    print(f\"\\nEvaluando funci√≥n de p√©rdida: {loss_func}\")\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss_func, metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if loss_func == 'categorical_crossentropy':\n",
    "        history = model.fit(x_train, y_train_onehot,\n",
    "                           epochs=10,\n",
    "                           batch_size=128,\n",
    "                           validation_split=0.2,\n",
    "                           verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "    else:\n",
    "        history = model.fit(x_train, y_train,\n",
    "                           epochs=10,\n",
    "                           batch_size=128,\n",
    "                           validation_split=0.2,\n",
    "                           verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = {\n",
    "        'loss_function': loss_func,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "        'tiempo_entrenamiento': tiempo_entrenamiento,\n",
    "        'history': history\n",
    "    }\n",
    "    resultados_loss.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "loss_names = [r['loss_function'].replace('_', '\\n') for r in resultados_loss]\n",
    "test_accs = [r['test_accuracy'] for r in resultados_loss]\n",
    "test_losses = [r['test_loss'] for r in resultados_loss]\n",
    "\n",
    "axes[0].bar(loss_names, test_accs, color='skyblue', alpha=0.7)\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Comparaci√≥n Accuracy por Loss Function')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(loss_names, test_losses, color='lightcoral', alpha=0.7)\n",
    "axes[1].set_ylabel('Test Loss')\n",
    "axes[1].set_title('Comparaci√≥n Loss por Loss Function')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mejor_loss = max(resultados_loss, key=lambda x: x['test_accuracy'])\n",
    "print(f\"\\nMejor funci√≥n de p√©rdida: {mejor_loss['loss_function']} (Accuracy: {mejor_loss['test_accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Estudio de batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: Batch Size\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "resultados_batch = []\n",
    "\n",
    "print(\"\\n=== ESTUDIO: BATCH SIZE ===\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nEvaluando batch size: {batch_size}\")\n",
    "    \n",
    "    model = crear_modelo(n_neuronas=128)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       epochs=10,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = evaluar_modelo(model, x_test, y_test,\n",
    "                              f\"Batch {batch_size}\",\n",
    "                              tiempo_entrenamiento)\n",
    "    resultado['batch_size'] = batch_size\n",
    "    resultado['val_accuracy'] = history.history['val_accuracy'][-1]\n",
    "    resultado['history'] = history\n",
    "    resultados_batch.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {resultado['test_accuracy']:.4f}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Estudio de learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: Learning Rate\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "resultados_lr = []\n",
    "\n",
    "print(\"\\n=== ESTUDIO: LEARNING RATE ===\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nEvaluando learning rate: {lr}\")\n",
    "    \n",
    "    model = crear_modelo(n_neuronas=128, learning_rate=lr)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       epochs=10,\n",
    "                       batch_size=128,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = evaluar_modelo(model, x_test, y_test,\n",
    "                              f\"LR {lr}\",\n",
    "                              tiempo_entrenamiento)\n",
    "    resultado['learning_rate'] = lr\n",
    "    resultado['val_accuracy'] = history.history['val_accuracy'][-1]\n",
    "    resultado['history'] = history\n",
    "    resultados_lr.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {resultado['test_accuracy']:.4f}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Estudio de porcentaje de validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio sistem√°tico: Validation Split\n",
    "validation_splits = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "resultados_val = []\n",
    "\n",
    "print(\"\\n=== ESTUDIO: PORCENTAJE DE VALIDACI√ìN ===\")\n",
    "\n",
    "for val_split in validation_splits:\n",
    "    print(f\"\\nEvaluando validation split: {val_split:.1%}\")\n",
    "    \n",
    "    model = crear_modelo(n_neuronas=128)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       epochs=10,\n",
    "                       batch_size=128,\n",
    "                       validation_split=val_split,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    resultado = evaluar_modelo(model, x_test, y_test,\n",
    "                              f\"Val {val_split:.1%}\",\n",
    "                              tiempo_entrenamiento)\n",
    "    resultado['val_split'] = val_split\n",
    "    resultado['val_accuracy'] = history.history['val_accuracy'][-1]\n",
    "    resultado['train_size'] = int(len(x_train) * (1 - val_split))\n",
    "    resultados_val.append(resultado)\n",
    "    \n",
    "    print(f\"  Test Accuracy: {resultado['test_accuracy']:.4f}\")\n",
    "    print(f\"  Tama√±o train: {resultado['train_size']}\")\n",
    "    print(f\"  Tiempo: {tiempo_entrenamiento:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clasificaci√≥n Par/Impar Optimizada\n",
    "\n",
    "Ahora creamos un modelo espec√≠ficamente dise√±ado para clasificar n√∫meros pares vs impares, usando los mejores hiperpar√°metros encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== CLASIFICACI√ìN PAR/IMPAR OPTIMIZADA ===\")\n",
    "\n",
    "# Grid search para encontrar la mejor configuraci√≥n par/impar\n",
    "param_grid = {\n",
    "    'n_neuronas': [64, 128, 256],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout': [0.1, 0.3, 0.5],\n",
    "    'epochs': [15, 20]\n",
    "}\n",
    "\n",
    "mejores_resultados = []\n",
    "contador = 0\n",
    "total_combinaciones = len(param_grid['n_neuronas']) * len(param_grid['learning_rate']) * len(param_grid['dropout']) * len(param_grid['epochs'])\n",
    "\n",
    "for n_neuronas, lr, dropout, epochs in product(param_grid['n_neuronas'], \n",
    "                                               param_grid['learning_rate'],\n",
    "                                               param_grid['dropout'],\n",
    "                                               param_grid['epochs']):\n",
    "    contador += 1\n",
    "    print(f\"\\rProgreso: {contador}/{total_combinaciones} ({contador/total_combinaciones*100:.1f}%)\", end='', flush=True)\n",
    "    \n",
    "    model = crear_modelo_parimpar(n_neuronas=n_neuronas, \n",
    "                                 learning_rate=lr, \n",
    "                                 dropout=dropout)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train_parimpar,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=128,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=0)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test_parimpar, verbose=0)\n",
    "    \n",
    "    resultado = {\n",
    "        'n_neuronas': n_neuronas,\n",
    "        'learning_rate': lr,\n",
    "        'dropout': dropout,\n",
    "        'epochs': epochs,\n",
    "        'test_accuracy': test_acc,\n",
    "        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "        'tiempo_entrenamiento': tiempo_entrenamiento,\n",
    "        'model': model\n",
    "    }\n",
    "    mejores_resultados.append(resultado)\n",
    "\n",
    "print(\"\\n\\nGrid Search completado!\")\n",
    "\n",
    "# Encontrar mejor configuraci√≥n\n",
    "mejor_config = max(mejores_resultados, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\n=== MEJOR CONFIGURACI√ìN PAR/IMPAR ===")\n",
    "print(f\"Neuronas: {mejor_config['n_neuronas']}\")\n",
    "print(f\"Learning Rate: {mejor_config['learning_rate']}\")\n",
    "print(f\"Dropout: {mejor_config['dropout']}\")\n",
    "print(f\"√âpocas: {mejor_config['epochs']}\")\n",
    "print(f\"Test Accuracy: {mejor_config['test_accuracy']:.4f}\")\n",
    "print(f\"Val Accuracy: {mejor_config['val_accuracy']:.4f}\")\n",
    "print(f\"Tiempo: {mejor_config['tiempo_entrenamiento']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n heatmap de resultados\n",
    "import pandas as pd\n",
    "\n",
    "# Crear DataFrame para an√°lisis\n",
    "df_resultados = pd.DataFrame(mejores_resultados)\n",
    "\n",
    "print(\"\\n=== TOP 10 CONFIGURACIONES ===\")\n",
    "top_10 = df_resultados.nlargest(10, 'test_accuracy')\n",
    "print(top_10[['n_neuronas', 'learning_rate', 'dropout', 'epochs', 'test_accuracy', 'tiempo_entrenamiento']].to_string(index=False))\n",
    "\n",
    "# Heatmap por hiperpar√°metros\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('An√°lisis de Hiperpar√°metros - Clasificaci√≥n Par/Impar', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Heatmap: Neuronas vs Learning Rate\n",
    "pivot1 = df_resultados.groupby(['n_neuronas', 'learning_rate'])['test_accuracy'].mean().unstack()\n",
    "sns.heatmap(pivot1, annot=True, fmt='.4f', cmap='YlOrRd', ax=axes[0,0])\n",
    "axes[0,0].set_title('Accuracy: Neuronas vs Learning Rate')\n",
    "\n",
    "# Heatmap: Dropout vs Epochs\n",
    "pivot2 = df_resultados.groupby(['dropout', 'epochs'])['test_accuracy'].mean().unstack()\n",
    "sns.heatmap(pivot2, annot=True, fmt='.4f', cmap='YlOrRd', ax=axes[0,1])\n",
    "axes[0,1].set_title('Accuracy: Dropout vs Epochs')\n",
    "\n",
    "# Distribuci√≥n accuracy\n",
    "axes[1,0].hist(df_resultados['test_accuracy'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].axvline(mejor_config['test_accuracy'], color='red', linestyle='--', \n",
    "                  label=f\"Mejor: {mejor_config['test_accuracy']:.4f}\")\n",
    "axes[1,0].set_xlabel('Test Accuracy')\n",
    "axes[1,0].set_ylabel('Frecuencia')\n",
    "axes[1,0].set_title('Distribuci√≥n de Accuracy')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tiempo vs Accuracy\n",
    "scatter = axes[1,1].scatter(df_resultados['tiempo_entrenamiento'], \n",
    "                           df_resultados['test_accuracy'],\n",
    "                           c=df_resultados['n_neuronas'], \n",
    "                           cmap='viridis', alpha=0.7, s=50)\n",
    "axes[1,1].set_xlabel('Tiempo Entrenamiento (s)')\n",
    "axes[1,1].set_ylabel('Test Accuracy')\n",
    "axes[1,1].set_title('Eficiencia: Tiempo vs Accuracy')\n",
    "plt.colorbar(scatter, ax=axes[1,1], label='Neuronas')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluaci√≥n detallada del mejor modelo Par/Impar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el mejor modelo con m√°s detalle\n",
    "print(\"=== ENTRENAMIENTO DEL MODELO √ìPTIMO ===")\n",
    "print(f\"Configuraci√≥n √≥ptima:\")\n",
    "print(f\"  - Neuronas: {mejor_config['n_neuronas']}\")\n",
    "print(f\"  - Learning Rate: {mejor_config['learning_rate']}\")\n",
    "print(f\"  - Dropout: {mejor_config['dropout']}\")\n",
    "print(f\"  - √âpocas: {mejor_config['epochs']}\")\n",
    "\n",
    "# Entrenar modelo final\n",
    "modelo_final = crear_modelo_parimpar(\n",
    "    n_neuronas=mejor_config['n_neuronas'],\n",
    "    learning_rate=mejor_config['learning_rate'],\n",
    "    dropout=mejor_config['dropout']\n",
    ")\n",
    "\n",
    "# Entrenamiento con callbacks avanzados\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "history_final = modelo_final.fit(\n",
    "    x_train, y_train_parimpar,\n",
    "    epochs=mejor_config['epochs'],\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "tiempo_final = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTiempo total de entrenamiento: {tiempo_final:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n detallada del modelo final\n",
    "test_loss, test_acc = modelo_final.evaluate(x_test, y_test_parimpar, verbose=0)\n",
    "train_loss, train_acc = modelo_final.evaluate(x_train, y_train_parimpar, verbose=0)\n",
    "\n",
    "y_pred = modelo_final.predict(x_test, verbose=0)\n",
    "y_pred_class = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"=== M√âTRICAS FINALES DEL MODELO √ìPTIMO ===")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test_parimpar, y_pred_class)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Evaluaci√≥n Detallada - Modelo Par/Impar √ìptimo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "           xticklabels=['Par', 'Impar'], yticklabels=['Par', 'Impar'])\n",
    "axes[0,0].set_xlabel('Predicho')\n",
    "axes[0,0].set_ylabel('Real')\n",
    "axes[0,0].set_title('Matriz de Confusi√≥n')\n",
    "\n",
    "# Curvas de entrenamiento\n",
    "epochs_range = range(1, len(history_final.history['accuracy']) + 1)\n",
    "axes[0,1].plot(epochs_range, history_final.history['accuracy'], 'b-', label='Train')\n",
    "axes[0,1].plot(epochs_range, history_final.history['val_accuracy'], 'r--', label='Validation')\n",
    "axes[0,1].set_xlabel('√âpocas')\n",
    "axes[0,1].set_ylabel('Accuracy')\n",
    "axes[0,1].set_title('Curvas de Aprendizaje')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuci√≥n de confianza en predicciones\n",
    "confianza = np.maximum(y_pred, 1-y_pred).flatten()\n",
    "axes[1,0].hist(confianza, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1,0].set_xlabel('Confianza de Predicci√≥n')\n",
    "axes[1,0].set_ylabel('Frecuencia')\n",
    "axes[1,0].set_title('Distribuci√≥n de Confianza')\n",
    "axes[1,0].axvline(np.mean(confianza), color='red', linestyle='--', \n",
    "                  label=f\"Media: {np.mean(confianza):.3f}\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# An√°lisis por d√≠gito\n",
    "accuracies_por_digito = []\n",
    "for digito in range(10):\n",
    "    mask = y_test == digito\n",
    "    if np.sum(mask) > 0:\n",
    "        acc_digito = np.mean(y_pred_class[mask] == y_test_parimpar[mask])\n",
    "        accuracies_por_digito.append(acc_digito)\n",
    "    else:\n",
    "        accuracies_por_digito.append(0)\n",
    "\n",
    "axes[1,1].bar(range(10), accuracies_por_digito, \n",
    "              color=['blue' if i%2==0 else 'orange' for i in range(10)], alpha=0.7)\n",
    "axes[1,1].set_xlabel('D√≠gito')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].set_title('Accuracy por D√≠gito (Azul=Par, Naranja=Impar)')\n",
    "axes[1,1].set_xticks(range(10))\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n detallado\n",
    "print(\"\\n=== REPORTE DE CLASIFICACI√ìN DETALLADO ===")\n",
    "print(classification_report(y_test_parimpar, y_pred_class, \n",
    "                           target_names=['Par', 'Impar'], digits=4))\n",
    "\n",
    "print(f\"\\nAccuracy por d√≠gito:\")\n",
    "for i, acc in enumerate(accuracies_por_digito):\n",
    "    tipo = 'Par' if i%2==0 else 'Impar'\n",
    "    print(f\"  D√≠gito {i} ({tipo}): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaci√≥n: Clasificaci√≥n 0-9 vs Par/Impar\n",
    "\n",
    "Comparamos la dificultad de ambas tareas usando arquitecturas similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPARACI√ìN: CLASIFICACI√ìN 0-9 vs PAR/IMPAR ===")\n",
    "\n",
    "# Modelo para clasificaci√≥n 0-9 con arquitectura similar\n",
    "modelo_09 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(mejor_config['n_neuronas'], activation='relu'),\n",
    "    tf.keras.layers.Dropout(mejor_config['dropout']),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "modelo_09.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=mejor_config['learning_rate']),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Entrenar ambos modelos\n",
    "print(\"\\nEntrenando modelo 0-9...\")\n",
    "start_time = time.time()\n",
    "history_09 = modelo_09.fit(x_train, y_train,\n",
    "                           epochs=mejor_config['epochs'],\n",
    "                           batch_size=128,\n",
    "                           validation_split=0.2,\n",
    "                           verbose=0)\n",
    "tiempo_09 = time.time() - start_time\n",
    "\n",
    "print(\"\\nEntrenando modelo Par/Impar...\")\n",
    "start_time = time.time()\n",
    "history_parimpar = modelo_final.fit(x_train, y_train_parimpar,\n",
    "                                   epochs=mejor_config['epochs'],\n",
    "                                   batch_size=128,\n",
    "                                   validation_split=0.2,\n",
    "                                   verbose=0)\n",
    "tiempo_parimpar = time.time() - start_time\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss_09, test_acc_09 = modelo_09.evaluate(x_test, y_test, verbose=0)\n",
    "test_loss_pi, test_acc_pi = modelo_final.evaluate(x_test, y_test_parimpar, verbose=0)\n",
    "\n",
    "print(f\"\\n=== RESULTADOS COMPARATIVOS ===")\n",
    "print(f\"Clasificaci√≥n 0-9:\")\n",
    "print(f\"  - Test Accuracy: {test_acc_09:.4f}\")\n",
    "print(f\"  - Tiempo: {tiempo_09:.2f}s\")\n",
    "print(f\"  - Complejidad: 10 clases\")\n",
    "\n",
    "print(f\"Clasificaci√≥n Par/Impar:\")\n",
    "print(f\"  - Test Accuracy: {test_acc_pi:.4f}\")\n",
    "print(f\"  - Tiempo: {tiempo_parimpar:.2f}s\")\n",
    "print(f\"  - Complejidad: 2 clases\")\n",
    "\n",
    "mejora_accuracy = test_acc_pi - test_acc_09\n",
    "print(f\"\\n‚úÖ La clasificaci√≥n Par/Impar es {mejora_accuracy:.4f} ({mejora_accuracy*100:.2f}%) m√°s precisa\")\n",
    "\n",
    "if mejora_accuracy > 0.02:\n",
    "    print(\"\\nüéØ CONCLUSI√ìN: El problema Par/Impar es SIGNIFICATIVAMENTE M√ÅS F√ÅCIL\")\n",
    "    print(\"   - Menos clases = mejor separabilidad\")\n",
    "    print(\"   - Patrones m√°s simples de reconocer\")\n",
    "    print(\"   - Mayor generalizaci√≥n\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è CONCLUSI√ìN: Ambos problemas tienen complejidad similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lisis de sensibilidad de hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de sensibilidad: ¬øQu√© tan cr√≠tico es cada hiperpar√°metro?\n",
    "print(\"=== AN√ÅLISIS DE SENSIBILIDAD ===")\n",
    "\n",
    "# Agrupar por cada hiperpar√°metro\n",
    "sensibilidad_neuronas = df_resultados.groupby('n_neuronas')['test_accuracy'].agg(['mean', 'std'])\n",
    "sensibilidad_lr = df_resultados.groupby('learning_rate')['test_accuracy'].agg(['mean', 'std'])\n",
    "sensibilidad_dropout = df_resultados.groupby('dropout')['test_accuracy'].agg(['mean', 'std'])\n",
    "sensibilidad_epochs = df_resultados.groupby('epochs')['test_accuracy'].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\nSENSIBILIDAD POR HIPERPAR√ÅMETRO:\")\n",
    "print(\"\\n1. NEURONAS:\")\n",
    "for idx, row in sensibilidad_neuronas.iterrows():\n",
    "    print(f\"   {idx:3d}: Œº={row['mean']:.4f}, œÉ={row['std']:.4f}\")\n",
    "\n",
    "print(\"\\n2. LEARNING RATE:\")\n",
    "for idx, row in sensibilidad_lr.iterrows():\n",
    "    print(f\"   {idx:.3f}: Œº={row['mean']:.4f}, œÉ={row['std']:.4f}\")\n",
    "\n",
    "print(\"\\n3. DROPOUT:\")\n",
    "for idx, row in sensibilidad_dropout.iterrows():\n",
    "    print(f\"   {idx:.1f}: Œº={row['mean']:.4f}, œÉ={row['std']:.4f}\")\n",
    "\n",
    "print(\"\\n4. √âPOCAS:\")\n",
    "for idx, row in sensibilidad_epochs.iterrows():\n",
    "    print(f\"   {idx:2d}: Œº={row['mean']:.4f}, œÉ={row['std']:.4f}\")\n",
    "\n",
    "# Calcular coeficiente de variaci√≥n (CV) para determinar sensibilidad\n",
    "cv_neuronas = (sensibilidad_neuronas['std'] / sensibilidad_neuronas['mean']).mean()\n",
    "cv_lr = (sensibilidad_lr['std'] / sensibilidad_lr['mean']).mean()\n",
    "cv_dropout = (sensibilidad_dropout['std'] / sensibilidad_dropout['mean']).mean()\n",
    "cv_epochs = (sensibilidad_epochs['std'] / sensibilidad_epochs['mean']).mean()\n",
    "\n",
    "print(\"\\n=== RANKING DE SENSIBILIDAD (mayor CV = m√°s cr√≠tico) ===")\n",
    "sensibilidades = [\n",
    "    ('Learning Rate', cv_lr),\n",
    "    ('Dropout', cv_dropout),\n",
    "    ('Neuronas', cv_neuronas),\n",
    "    ('√âpocas', cv_epochs)\n",
    "]\n",
    "\n",
    "sensibilidades.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (param, cv) in enumerate(sensibilidades, 1):\n",
    "    print(f\"{i}. {param}: CV = {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones y Recomendaciones\n",
    "\n",
    "### Impacto de cada hiperpar√°metro:\n",
    "\n",
    "1. **N√∫mero de neuronas**: M√°s neuronas = mayor capacidad, pero risk overfitting\n",
    "2. **N√∫mero de √©pocas**: Convergencia gradual, early stopping recomendado\n",
    "3. **Funci√≥n de p√©rdida**: sparse_categorical_crossentropy es m√°s eficiente\n",
    "4. **Batch size**: Compromiso entre estabilidad y velocidad\n",
    "5. **Learning rate**: Par√°metro m√°s cr√≠tico para convergencia\n",
    "6. **Validaci√≥n**: 20% proporciona buen balance\n",
    "\n",
    "### Clasificaci√≥n Par/Impar:\n",
    "- **M√ÅS F√ÅCIL** que clasificaci√≥n 0-9\n",
    "- Requiere menos neuronas para mismo rendimiento\n",
    "- Converge m√°s r√°pido\n",
    "- Mayor accuracy final\n",
    "\n",
    "### Configuraci√≥n √≥ptima recomendada:\n",
    "- **Neuronas**: 128-256 (balance capacidad/eficiencia)\n",
    "- **√âpocas**: 15-20 con early stopping\n",
    "- **Learning rate**: 0.001 (Adam)\n",
    "- **Dropout**: 0.3 (regularizaci√≥n √≥ptima)\n",
    "- **Batch size**: 128 (estabilidad)\n",
    "- **Validaci√≥n**: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen final\n",
    "print(\"\\n=== TABLA RESUMEN FINAL ===")\n",
    "print(\"\\nMejores resultados por hiperpar√°metro:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Mejor # Neuronas: {max(resultados_neuronas, key=lambda x: x['test_accuracy'])['n_neuronas']} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_neuronas, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"Mejor # √âpocas: {max(resultados_epochs, key=lambda x: x['test_accuracy'])['n_epochs']} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_epochs, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"Mejor Loss: {max(resultados_loss, key=lambda x: x['test_accuracy'])['loss_function']} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_loss, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"Mejor Batch Size: {max(resultados_batch, key=lambda x: x['test_accuracy'])['batch_size']} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_batch, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"Mejor Learning Rate: {max(resultados_lr, key=lambda x: x['test_accuracy'])['learning_rate']} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_lr, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"Mejor Val Split: {max(resultados_val, key=lambda x: x['test_accuracy'])['val_split']:.1%} \", end='')\n",
    "print(f\"(Accuracy: {max(resultados_val, key=lambda x: x['test_accuracy'])['test_accuracy']:.4f})\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"üèÜ MODELO √ìPTIMO PAR/IMPAR: {mejor_config['test_accuracy']:.4f} accuracy\")\n",
    "print(f\"üìä MEJORA vs clasificaci√≥n 0-9: +{(test_acc_pi - test_acc_09)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}