{\n  "nbformat": 4,\n  "nbformat_minor": 0,\n  "metadata": {\n    "colab": {\n      "provenance": [],\n      "gpuType": "T4"\n    },\n    "kernelspec": {\n      "name": "python3",\n      "display_name": "Python 3"\n    },\n    "language_info": {\n      "name": "python"\n    },\n    "accelerator": "GPU"\n  },\n  "cells": [\n    {\n      "cell_type": "markdown",\n      "metadata": {\n        "id": "title_cell"\n      },\n      "source": [\n        "# PrÃ¡ctica 0: ImplementaciÃ³n de PerceptrÃ³n Multicapa desde Cero\\n",\n        "\\n",\n        "ImplementaciÃ³n de un perceptrÃ³n multicapa (MLP) sin usar frameworks de deep learning, solo NumPy.\\n",\n        "\\n",\n        "**Objetivos:**\\n",\n        "- Implementar forward pass y backpropagation manualmente\\n",\n        "- Probar con XOR (problema no linealmente separable)\\n",\n        "- Aplicar a problema de regresiÃ³n"\n      ]\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# ConfiguraciÃ³n inicial\\n",\n        "import numpy as np\\n",\n        "import matplotlib.pyplot as plt\\n",\n        "import pandas as pd\\n",\n        "import time\\n",\n        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\\n",\n        "\\n",\n        "np.random.seed(42)\\n",\n        "plt.style.use('seaborn-v0_8')\\n",\n        "\\n",\n        "print(\\"ConfiguraciÃ³n inicial completada\\")"\n      ],\n      "metadata": {\n        "id": "setup_cell"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# ImplementaciÃ³n del PerceptrÃ³n Multicapa\\n",\n        "class MultiLayerPerceptron:\\n",\n        "    def __init__(self, input_size, hidden_size, output_size, \\n",\n        "                 learning_rate=0.01, epochs=5000, activation='relu', task='classification'):\\n",\n        "        # InicializaciÃ³n Xavier\\n",\n        "        xavier_std = np.sqrt(2.0 / (input_size + hidden_size))\\n",\n        "        self.W1 = np.random.normal(0, xavier_std, (input_size, hidden_size))\\n",\n        "        self.b1 = np.zeros((1, hidden_size))\\n",\n        "        \\n",\n        "        xavier_std2 = np.sqrt(2.0 / (hidden_size + output_size))\\n",\n        "        self.W2 = np.random.normal(0, xavier_std2, (hidden_size, output_size))\\n",\n        "        self.b2 = np.zeros((1, output_size))\\n",\n        "        \\n",\n        "        self.learning_rate = learning_rate\\n",\n        "        self.epochs = epochs\\n",\n        "        self.activation = activation\\n",\n        "        self.task = task\\n",\n        "        \\n",\n        "        self.train_errors = []\\n",\n        "        self.validation_errors = []\\n",\n        "    \\n",\n        "    def _activation_function(self, x):\\n",\n        "        if self.activation == 'relu':\\n",\n        "            return np.maximum(0, x)\\n",\n        "        elif self.activation == 'sigmoid':\\n",\n        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\\n",\n        "        elif self.activation == 'tanh':\\n",\n        "            return np.tanh(x)\\n",\n        "    \\n",\n        "    def _activation_derivative(self, x):\\n",\n        "        if self.activation == 'relu':\\n",\n        "            return (x > 0).astype(float)\\n",\n        "        elif self.activation == 'sigmoid':\\n",\n        "            return x * (1 - x)\\n",\n        "        elif self.activation == 'tanh':\\n",\n        "            return 1 - x ** 2\\n",\n        "    \\n",\n        "    def _output_function(self, x):\\n",\n        "        if self.task == 'classification':\\n",\n        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\\n",\n        "        else:\\n",\n        "            return x  # Lineal para regresiÃ³n\\n",\n        "    \\n",\n        "    def _output_derivative(self, x):\\n",\n        "        if self.task == 'classification':\\n",\n        "            return x * (1 - x)\\n",\n        "        else:\\n",\n        "            return np.ones_like(x)\\n",\n        "    \\n",\n        "    def forward(self, X):\\n",\n        "        # Capa oculta\\n",\n        "        self.z1 = np.dot(X, self.W1) + self.b1\\n",\n        "        self.a1 = self._activation_function(self.z1)\\n",\n        "        \\n",\n        "        # Capa de salida\\n",\n        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\\n",\n        "        self.output = self._output_function(self.z2)\\n",\n        "        \\n",\n        "        return self.output\\n",\n        "    \\n",\n        "    def backward(self, X, y):\\n",\n        "        m = X.shape[0]\\n",\n        "        \\n",\n        "        # Error capa de salida\\n",\n        "        error = y - self.output\\n",\n        "        d_output = error * self._output_derivative(self.output)\\n",\n        "        \\n",\n        "        # Error capa oculta\\n",\n        "        error_hidden = d_output.dot(self.W2.T)\\n",\n        "        d_hidden = error_hidden * self._activation_derivative(self.a1)\\n",\n        "        \\n",\n        "        # Actualizar pesos\\n",\n        "        self.W2 += (self.a1.T.dot(d_output) / m) * self.learning_rate\\n",\n        "        self.b2 += np.mean(d_output, axis=0, keepdims=True) * self.learning_rate\\n",\n        "        self.W1 += (X.T.dot(d_hidden) / m) * self.learning_rate\\n",\n        "        self.b1 += np.mean(d_hidden, axis=0, keepdims=True) * self.learning_rate\\n",\n        "    \\n",\n        "    def fit(self, X, y, X_val=None, y_val=None, verbose=True):\\n",\n        "        start_time = time.time()\\n",\n        "        \\n",\n        "        for epoch in range(self.epochs):\\n",\n        "            self.forward(X)\\n",\n        "            self.backward(X, y)\\n",\n        "            \\n",\n        "            # Registrar errores\\n",\n        "            mse_train = np.mean((y - self.output) ** 2)\\n",\n        "            self.train_errors.append(mse_train)\\n",\n        "            \\n",\n        "            if X_val is not None and y_val is not None:\\n",\n        "                val_pred = self.forward(X_val)\\n",\n        "                mse_val = np.mean((y_val - val_pred) ** 2)\\n",\n        "                self.validation_errors.append(mse_val)\\n",\n        "            \\n",\n        "            # Mostrar progreso\\n",\n        "            if verbose and (epoch % (self.epochs // 10) == 0 or epoch == self.epochs - 1):\\n",\n        "                if X_val is not None:\\n",\n        "                    print(f\\"Ã‰poca {epoch:4d}: Train MSE = {mse_train:.6f}, Val MSE = {mse_val:.6f}\\")\\n",\n        "                else:\\n",\n        "                    print(f\\"Ã‰poca {epoch:4d}: MSE = {mse_train:.6f}\\")\\n",\n        "        \\n",\n        "        return time.time() - start_time\\n",\n        "    \\n",\n        "    def predict(self, X):\\n",\n        "        output = self.forward(X)\\n",\n        "        if self.task == 'classification':\\n",\n        "            return (output > 0.5).astype(int)\\n",\n        "        else:\\n",\n        "            return output\\n",\n        "\\n",\n        "print(\\"Clase MultiLayerPerceptron definida\\")"\n      ],\n      "metadata": {\n        "id": "mlp_class"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Problema XOR - verificaciÃ³n del MLP\\n",\n        "print(\\"=== PROBLEMA XOR ===\\")\\n",\n        "\\n",\n        "# Datos XOR\\n",\n        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n",\n        "y_xor = np.array([[0], [1], [1], [0]])\\n",\n        "\\n",\n        "print(\\"Datos XOR:\\")\\n",\n        "for i in range(len(X_xor)):\\n",\n        "    print(f\\"  {X_xor[i]} -> {y_xor[i][0]}\\")\\n",\n        "\\n",\n        "# Configuraciones a probar\\n",\n        "configs_xor = [\\n",\n        "    {'hidden_size': 2, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\\n",\n        "    {'hidden_size': 4, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\\n",\n        "    {'hidden_size': 8, 'learning_rate': 0.05, 'epochs': 2000, 'activation': 'relu'},\\n",\n        "    {'hidden_size': 10, 'learning_rate': 0.01, 'epochs': 5000, 'activation': 'tanh'}\\n",\n        "]\\n",\n        "\\n",\n        "print(\\"\\\\nEntrenando diferentes configuraciones:\\")\\n",\n        "\\n",\n        "mejor_accuracy = 0\\n",\n        "mejor_modelo_xor = None\\n",\n        "\\n",\n        "for i, config in enumerate(configs_xor):\\n",\n        "    print(f\\"Config {i+1}: {config}\\")\\n",\n        "    \\n",\n        "    mlp = MultiLayerPerceptron(\\n",\n        "        input_size=2,\\n",\n        "        output_size=1,\\n",\n        "        task='classification',\\n",\n        "        **config\\n",\n        "    )\\n",\n        "    \\n",\n        "    tiempo = mlp.fit(X_xor, y_xor, verbose=False)\\n",\n        "    predicciones = mlp.predict(X_xor)\\n",\n        "    accuracy = np.mean(y_xor == predicciones)\\n",\n        "    \\n",\n        "    print(f\\"  Tiempo: {tiempo:.3f}s | Accuracy: {accuracy:.4f}\\")\\n",\n        "    print(f\\"  Predicciones: {predicciones.flatten()}\\")\\n",\n        "    \\n",\n        "    if accuracy > mejor_accuracy:\\n",\n        "        mejor_accuracy = accuracy\\n",\n        "        mejor_modelo_xor = mlp\\n",\n        "    \\n",\n        "    print()\\n",\n        "\\n",\n        "print(f\\"Mejor accuracy XOR: {mejor_accuracy:.4f}\\")"\n      ],\n      "metadata": {\n        "id": "xor_problem"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Visualizar frontera de decisiÃ³n XOR\\n",\n        "def plot_decision_boundary(model, X, y, title=\\"Frontera de DecisiÃ³n\\"):\\n",\n        "    h = 0.01\\n",\n        "    x_min, x_max = -0.1, 1.1\\n",\n        "    y_min, y_max = -0.1, 1.1\\n",\n        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n",\n        "                         np.arange(y_min, y_max, h))\\n",\n        "    \\n",\n        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\\n",\n        "    Z = model.forward(mesh_points)\\n",\n        "    Z = Z.reshape(xx.shape)\\n",\n        "    \\n",\n        "    plt.figure(figsize=(8, 6))\\n",\n        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdBu')\\n",\n        "    plt.colorbar(label='Salida')\\n",\n        "    \\n",\n        "    colors = ['red' if label == 0 else 'blue' for label in y.flatten()]\\n",\n        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\\n",\n        "    \\n",\n        "    plt.xlabel('X1')\\n",\n        "    plt.ylabel('X2')\\n",\n        "    plt.title(title)\\n",\n        "    plt.grid(True, alpha=0.3)\\n",\n        "    plt.show()\\n",\n        "\\n",\n        "# Curva de aprendizaje\\n",\n        "plt.figure(figsize=(10, 4))\\n",\n        "plt.plot(mejor_modelo_xor.train_errors, linewidth=2)\\n",\n        "plt.xlabel('Ã‰poca')\\n",\n        "plt.ylabel('MSE')\\n",\n        "plt.title('Curva de Aprendizaje - XOR')\\n",\n        "plt.yscale('log')\\n",\n        "plt.grid(True, alpha=0.3)\\n",\n        "plt.show()\\n",\n        "\\n",\n        "plot_decision_boundary(mejor_modelo_xor, X_xor, y_xor, \\"Frontera de DecisiÃ³n XOR\\")"\n      ],\n      "metadata": {\n        "id": "xor_visualization"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Problema de RegresiÃ³n - DATOS MEJORADOS\\n",\n        "print(\\"=== PROBLEMA DE REGRESIÃ“N ===\\")\\n",\n        "\\n",\n        "# Generar datos sintÃ©ticos mÃ¡s apropiados\\n",\n        "print(\\"Generando datos sintÃ©ticos mejorados...\\")\\n",\n        "X_range = np.linspace(-2, 2, 600)\\n",\n        "# FunciÃ³n mÃ¡s simple pero interesante: parÃ¡bola + seno suave\\n",\n        "y_function = 0.8 * X_range**2 + 0.6 * np.sin(2 * X_range) + 0.2 * X_range + np.random.normal(0, 0.15, 600)\\n",\n        "\\n",\n        "split = int(0.8 * len(X_range))\\n",\n        "X_train = X_range[:split].reshape(-1, 1)\\n",\n        "y_train = y_function[:split].reshape(-1, 1)\\n",\n        "X_test = X_range[split:].reshape(-1, 1)\\n",\n        "y_test = y_function[split:].reshape(-1, 1)\\n",\n        "\\n",\n        "print(f\\"Train: {X_train.shape[0]} muestras\\")\\n",\n        "print(f\\"Test: {X_test.shape[0]} muestras\\")\\n",\n        "\\n",\n        "# NormalizaciÃ³n estÃ¡ndar\\n",\n        "X_mean, X_std = X_train.mean(), X_train.std()\\n",\n        "y_mean, y_std = y_train.mean(), y_train.std()\\n",\n        "\\n",\n        "X_train_norm = (X_train - X_mean) / X_std\\n",\n        "X_test_norm = (X_test - X_mean) / X_std\\n",\n        "y_train_norm = (y_train - y_mean) / y_std\\n",\n        "y_test_norm = (y_test - y_mean) / y_std\\n",\n        "\\n",\n        "print(f\\"NormalizaciÃ³n aplicada:\\")\\n",\n        "print(f\\"  X: mean={X_mean:.3f}, std={X_std:.3f}\\")\\n",\n        "print(f\\"  Y: mean={y_mean:.3f}, std={y_std:.3f}\\")\\n",\n        "print(f\\"  Rango Y: [{y_train.min():.2f}, {y_train.max():.2f}]\\")"\n      ],\n      "metadata": {\n        "id": "regression_data"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# BÃºsqueda de hiperparÃ¡metros para regresiÃ³n - CONFIGURACIONES MEJORADAS\\n",\n        "configs_regression = [\\n",\n        "    {'hidden_size': 20, 'learning_rate': 0.1, 'epochs': 2000, 'activation': 'tanh'},\\n",\n        "    {'hidden_size': 32, 'learning_rate': 0.08, 'epochs': 2500, 'activation': 'tanh'},\\n",\n        "    {'hidden_size': 16, 'learning_rate': 0.12, 'epochs': 1800, 'activation': 'sigmoid'},\\n",\n        "    {'hidden_size': 28, 'learning_rate': 0.09, 'epochs': 2200, 'activation': 'tanh'},\\n",\n        "    {'hidden_size': 24, 'learning_rate': 0.1, 'epochs': 2000, 'activation': 'sigmoid'},\\n",\n        "    {'hidden_size': 40, 'learning_rate': 0.06, 'epochs': 3000, 'activation': 'tanh'}\\n",\n        "]\\n",\n        "\\n",\n        "print(\\"Probando configuraciones para regresiÃ³n:\\")\\n",\n        "\\n",\n        "resultados = []\\n",\n        "mejor_r2 = -float('inf')\\n",\n        "mejor_modelo_reg = None\\n",\n        "\\n",\n        "for i, config in enumerate(configs_regression):\\n",\n        "    print(f\\"Config {i+1}: {config}\\")\\n",\n        "    \\n",\n        "    mlp = MultiLayerPerceptron(\\n",\n        "        input_size=1,\\n",\n        "        output_size=1,\\n",\n        "        task='regression',\\n",\n        "        **config\\n",\n        "    )\\n",\n        "    \\n",\n        "    tiempo = mlp.fit(X_train_norm, y_train_norm, verbose=False)\\n",\n        "    \\n",\n        "    # Evaluar en datos normalizados y desnormalizar\\n",\n        "    y_pred_norm = mlp.forward(X_test_norm)\\n",\n        "    y_pred = y_pred_norm * y_std + y_mean\\n",\n        "    \\n",\n        "    mse = mean_squared_error(y_test, y_pred)\\n",\n        "    r2 = r2_score(y_test, y_pred)\\n",\n        "    mae = mean_absolute_error(y_test, y_pred)\\n",\n        "    \\n",\n        "    resultados.append({\\n",\n        "        'config': config,\\n",\n        "        'modelo': mlp,\\n",\n        "        'tiempo': tiempo,\\n",\n        "        'mse': mse,\\n",\n        "        'r2': r2,\\n",\n        "        'mae': mae\\n",\n        "    })\\n",\n        "    \\n",\n        "    print(f\\"  Tiempo: {tiempo:.3f}s | MSE: {mse:.4f} | R2: {r2:.4f} | MAE: {mae:.4f}\\")\\n",\n        "    \\n",\n        "    if r2 > mejor_r2:\\n",\n        "        mejor_r2 = r2\\n",\n        "        mejor_modelo_reg = mlp\\n",\n        "    \\n",\n        "    print()\\n",\n        "\\n",\n        "print(f\\"Mejor R2 Score: {mejor_r2:.4f}\\")"\n      ],\n      "metadata": {\n        "id": "regression_training"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# VisualizaciÃ³n de resultados de regresiÃ³n - ARREGLADA\\n",\n        "\\n",\n        "# Obtener mejores predicciones\\n",\n        "y_pred_norm_best = mejor_modelo_reg.forward(X_test_norm)\\n",\n        "y_pred_best = y_pred_norm_best * y_std + y_mean\\n",\n        "\\n",\n        "# Curva de aprendizaje\\n",\n        "plt.figure(figsize=(10, 4))\\n",\n        "plt.plot(mejor_modelo_reg.train_errors, linewidth=2, color='blue')\\n",\n        "plt.xlabel('Ã‰poca')\\n",\n        "plt.ylabel('MSE (normalizado)')\\n",\n        "plt.title('Curva de Aprendizaje - RegresiÃ³n')\\n",\n        "plt.yscale('log')\\n",\n        "plt.grid(True, alpha=0.3)\\n",\n        "plt.show()\\n",\n        "\\n",\n        "# GrÃ¡ficas de evaluaciÃ³n\\n",\n        "plt.figure(figsize=(15, 5))\\n",\n        "\\n",\n        "# 1. Scatter plot: Predicciones vs Reales\\n",\n        "plt.subplot(1, 3, 1)\\n",\n        "plt.scatter(y_test, y_pred_best, alpha=0.7, s=25, color='blue')\\n",\n        "# LÃ­nea diagonal perfecta\\n",\n        "min_val, max_val = min(y_test.min(), y_pred_best.min()), max(y_test.max(), y_pred_best.max())\\n",\n        "plt.plot([min_val, max_val], [min_val, max_val], 'r-', lw=2, label='Perfecto')\\n",\n        "plt.xlabel('Valores Reales')\\n",\n        "plt.ylabel('Predicciones')\\n",\n        "plt.title(f'Predicciones vs Reales\\\\nRÂ² = {r2_score(y_test, y_pred_best):.4f}')\\n",\n        "plt.legend()\\n",\n        "plt.grid(True, alpha=0.3)\\n",\n        "\\n",\n        "# 2. FunciÃ³n vs aproximaciÃ³n\\n",\n        "plt.subplot(1, 3, 2)\\n",\n        "indices = np.argsort(X_test.flatten())\\n",\n        "plt.plot(X_test[indices], y_test[indices], 'b-', label='Real', linewidth=2.5)\\n",\n        "plt.plot(X_test[indices], y_pred_best[indices], 'r--', label='PredicciÃ³n', linewidth=2)\\n",\n        "plt.xlabel('X')\\n",\n        "plt.ylabel('Y')\\n",\n        "plt.title('FunciÃ³n vs AproximaciÃ³n')\\n",\n        "plt.legend()\\n",\n        "plt.grid(True, alpha=0.3)\\n",\n        "\\n",\n        "# 3. DistribuciÃ³n de Residuos\\n",\n        "plt.subplot(1, 3, 3)\\n",\n        "residuos = (y_test - y_pred_best).flatten()\\n",\n        "plt.hist(residuos, bins=25, alpha=0.7, color='green', edgecolor='black', density=True)\\n",\n        "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Ideal (0)')\\n",\n        "plt.axvline(x=residuos.mean(), color='orange', linestyle='-', linewidth=2, \\n",\n        "           label=f'Media={residuos.mean():.3f}')\\n",\n        "plt.xlabel('Residuos')\\n",\n        "plt.ylabel('Densidad')\\n",\n        "plt.title('DistribuciÃ³n de Residuos')\\n",\n        "plt.legend()\\n",\n        "plt.grid(True, alpha=0.3)\\n",\n        "\\n",\n        "plt.tight_layout()\\n",\n        "plt.show()\\n",\n        "\\n",\n        "# MÃ©tricas finales\\n",\n        "mse_final = mean_squared_error(y_test, y_pred_best)\\n",\n        "mae_final = mean_absolute_error(y_test, y_pred_best)\\n",\n        "r2_final = r2_score(y_test, y_pred_best)\\n",\n        "\\n",\n        "print(f\\"\\\\nMÃ©tricas del mejor modelo:\\")\\n",\n        "print(f\\"  MSE: {mse_final:.4f}\\")\\n",\n        "print(f\\"  MAE: {mae_final:.4f}\\")\\n",\n        "print(f\\"  RÂ²: {r2_final:.4f}\\")\\n",\n        "print(f\\"  RMSE: {np.sqrt(mse_final):.4f}\\")"\n      ],\n      "metadata": {\n        "id": "regression_visualization"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Tabla comparativa - VISUALIZACIÃ“N CORREGIDA\\n",\n        "df_resultados = pd.DataFrame([\\n",\n        "    {\\n",\n        "        'Config': i+1,\\n",\n        "        'Hidden Size': r['config']['hidden_size'],\\n",\n        "        'Learning Rate': r['config']['learning_rate'],\\n",\n        "        'Ã‰pocas': r['config']['epochs'],\\n",\n        "        'ActivaciÃ³n': r['config']['activation'],\\n",\n        "        'Tiempo (s)': f\\"{r['tiempo']:.3f}\\",\\n",\n        "        'MSE': f\\"{r['mse']:.4f}\\",\\n",\n        "        'RÂ² Score': f\\"{r['r2']:.4f}\\",\\n",\n        "        'MAE': f\\"{r['mae']:.4f}\\"\\n",\n        "    }\\n",\n        "    for i, r in enumerate(resultados)\\n",\n        "])\\n",\n        "\\n",\n        "# Ordenar por RÂ² descendente\\n",\n        "df_resultados = df_resultados.sort_values('RÂ² Score', ascending=False)\\n",\n        "\\n",\n        "print(\\"\\\\nComparaciÃ³n de configuraciones (ordenadas por RÂ²):\\")\\n",\n        "print(df_resultados.to_string(index=False))\\n",\n        "\\n",\n        "# VisualizaciÃ³n comparativa CORREGIDA\\n",\n        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\\n",\n        "\\n",\n        "configs_num = range(1, len(resultados) + 1)\\n",\n        "r2_scores = [r['r2'] for r in resultados]\\n",\n        "mse_scores = [r['mse'] for r in resultados]\\n",\n        "tiempos = [r['tiempo'] for r in resultados]\\n",\n        "\\n",\n        "# 1. RÂ² Score (CORREGIDO: valores positivos, hacia arriba es mejor)\\n",\n        "colors_r2 = ['darkgreen' if r2 > 0.7 else 'green' if r2 > 0.5 else 'orange' if r2 > 0.3 else 'red' for r2 in r2_scores]\\n",\n        "bars = axes[0].bar(configs_num, r2_scores, color=colors_r2, alpha=0.8, edgecolor='black')\\n",\n        "axes[0].set_title('RÂ² Score por ConfiguraciÃ³n', fontweight='bold')\\n",\n        "axes[0].set_xlabel('ConfiguraciÃ³n')\\n",\n        "axes[0].set_ylabel('RÂ² Score')\\n",\n        "axes[0].grid(True, alpha=0.3)\\n",\n        "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Baseline')\\n",\n        "# AÃ±adir valores en las barras\\n",\n        "for bar, r2 in zip(bars, r2_scores):\\n",\n        "    height = bar.get_height()\\n",\n        "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02 if height > 0 else height - 0.05,\\n",\n        "                f'{r2:.3f}', ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\\n",\n        "axes[0].legend()\\n",\n        "\\n",\n        "# 2. MSE (mÃ¡s bajo es mejor)\\n",\n        "colors_mse = ['darkgreen' if mse < 1 else 'green' if mse < 2 else 'orange' if mse < 4 else 'red' for mse in mse_scores]\\n",\n        "axes[1].bar(configs_num, mse_scores, color=colors_mse, alpha=0.8, edgecolor='black')\\n",\n        "axes[1].set_title('MSE por ConfiguraciÃ³n', fontweight='bold')\\n",\n        "axes[1].set_xlabel('ConfiguraciÃ³n')\\n",\n        "axes[1].set_ylabel('MSE')\\n",\n        "axes[1].set_yscale('log')\\n",\n        "axes[1].grid(True, alpha=0.3)\\n",\n        "\\n",\n        "# 3. Tiempo de entrenamiento\\n",\n        "axes[2].bar(configs_num, tiempos, color='steelblue', alpha=0.8, edgecolor='black')\\n",\n        "axes[2].set_title('Tiempo de Entrenamiento', fontweight='bold')\\n",\n        "axes[2].set_xlabel('ConfiguraciÃ³n')\\n",\n        "axes[2].set_ylabel('Tiempo (s)')\\n",\n        "axes[2].grid(True, alpha=0.3)\\n",\n        "\\n",\n        "plt.tight_layout()\\n",\n        "plt.show()\\n",\n        "\\n",\n        "# Resumen de interpretaciÃ³n\\n",\n        "best_r2 = max(r2_scores)\\n",\n        "best_config_idx = r2_scores.index(best_r2)\\n",\n        "\\n",\n        "print(f\\"\\\\nğŸ“Š INTERPRETACIÃ“N DE RESULTADOS:\\")\\n",\n        "print(f\\"  â€¢ Mejor RÂ²: {best_r2:.4f} (Config {best_config_idx+1})\\")\\n",\n        "if best_r2 > 0.8:\\n",\n        "    print(f\\"  âœ… Excelente ajuste (RÂ² > 0.8)\\")\\n",\n        "elif best_r2 > 0.6:\\n",\n        "    print(f\\"  âœ… Buen ajuste (RÂ² > 0.6)\\")\\n",\n        "elif best_r2 > 0.3:\\n",\n        "    print(f\\"  âš ï¸ Ajuste moderado (RÂ² > 0.3)\\")\\n",\n        "else:\\n",\n        "    print(f\\"  âŒ Ajuste pobre (RÂ² < 0.3)\\")\\n",\n        "\\n",\n        "print(f\\"  â€¢ Rango RÂ²: [{min(r2_scores):.4f}, {max(r2_scores):.4f}]\\")\\n",\n        "print(f\\"  â€¢ MSE promedio: {np.mean(mse_scores):.4f}\\")"\n      ],\n      "metadata": {\n        "id": "comparison_table"\n      },\n      "execution_count": null,\n      "outputs": []\n    }\n  ]\n}