{
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Práctica 0: Implementación de Perceptrón Multicapa desde Cero\n",
        "\n",
        "Implementación de un perceptrón multicapa (MLP) sin usar frameworks de deep learning, solo NumPy.\n",
        "\n",
        "**Objetivos:**\n",
        "- Implementar forward pass y backpropagation manualmente\n",
        "- Probar con XOR (problema no linealmente separable)\n",
        "- Aplicar a problema de regresión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración inicial\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementación del Perceptrón Multicapa\n",
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                 learning_rate=0.01, epochs=5000, activation='relu', task='classification'):\n",
        "        # Inicialización Xavier\n",
        "        xavier_std = np.sqrt(2.0 / (input_size + hidden_size))\n",
        "        self.W1 = np.random.normal(0, xavier_std, (input_size, hidden_size))\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        \n",
        "        xavier_std2 = np.sqrt(2.0 / (hidden_size + output_size))\n",
        "        self.W2 = np.random.normal(0, xavier_std2, (hidden_size, output_size))\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.activation = activation\n",
        "        self.task = task\n",
        "        \n",
        "        self.train_errors = []\n",
        "        self.validation_errors = []\n",
        "    \n",
        "    def _activation_function(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "        elif self.activation == 'tanh':\n",
        "            return np.tanh(x)\n",
        "    \n",
        "    def _activation_derivative(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return (x > 0).astype(float)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return x * (1 - x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return 1 - x ** 2\n",
        "    \n",
        "    def _output_function(self, x):\n",
        "        if self.task == 'classification':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "    def _output_derivative(self, x):\n",
        "        if self.task == 'classification':\n",
        "            return x * (1 - x)\n",
        "        else:\n",
        "            return np.ones_like(x)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # Capa oculta\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self._activation_function(self.z1)\n",
        "        \n",
        "        # Capa de salida\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.output = self._output_function(self.z2)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        # Error capa de salida\n",
        "        error = y - self.output\n",
        "        d_output = error * self._output_derivative(self.output)\n",
        "        # Error capa oculta\n",
        "        error_hidden = d_output.dot(self.W2.T)\n",
        "        d_hidden = error_hidden * self._activation_derivative(self.a1)\n",
        "        # Actualizar pesos\n",
        "        self.W2 += (self.a1.T.dot(d_output) / m) * self.learning_rate\n",
        "        self.b2 += np.mean(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.W1 += (X.T.dot(d_hidden) / m) * self.learning_rate\n",
        "        self.b1 += np.mean(d_hidden, axis=0, keepdims=True) * self.learning_rate\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None, verbose=True):\n",
        "        start_time = time.time()\n",
        "        for epoch in range(self.epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y)\n",
        "            mse_train = np.mean((y - self.output) ** 2)\n",
        "            self.train_errors.append(mse_train)\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self.forward(X_val)\n",
        "                mse_val = np.mean((y_val - val_pred) ** 2)\n",
        "                self.validation_errors.append(mse_val)\n",
        "        return time.time() - start_time\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        if self.task == 'classification':\n",
        "            return (output > 0.5).astype(int)\n",
        "        else:\n",
        "            return output\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Problema XOR - verificación del MLP\n",
        "# Datos XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "# Configs a probar\n",
        "configs_xor = [\n",
        "    {'hidden_size': 2, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 4, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 8, 'learning_rate': 0.05, 'epochs': 2000, 'activation': 'relu'},\n",
        "    {'hidden_size': 10, 'learning_rate': 0.01, 'epochs': 5000, 'activation': 'tanh'}\n",
        "]\n",
        "mejor_accuracy = 0\n",
        "mejor_modelo_xor = None\n",
        "for config in configs_xor:\n",
        "    mlp = MultiLayerPerceptron(input_size=2, output_size=1, task='classification', **config)\n",
        "    mlp.fit(X_xor, y_xor, verbose=False)\n",
        "    pred = mlp.predict(X_xor)\n",
        "    acc = np.mean(y_xor == pred)\n",
        "    if acc > mejor_accuracy:\n",
        "        mejor_accuracy = acc\n",
        "        mejor_modelo_xor = mlp\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización frontera XOR y curva de aprendizaje\n",
        "def plot_decision_boundary(model, X, y, title=\"Frontera de Decisión\"):\n",
        "    h = 0.01\n",
        "    x_min, x_max = -0.1, 1.1\n",
        "    y_min, y_max = -0.1, 1.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.forward(mesh_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdBu')\n",
        "    plt.colorbar(label='Salida')\n",
        "    colors = ['red' if label == 0 else 'blue' for label in y.flatten()]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mejor_modelo_xor.train_errors, linewidth=2)\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Curva de Aprendizaje - XOR')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "plot_decision_boundary(mejor_modelo_xor, X_xor, y_xor, \"Frontera de Decisión XOR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Problema de Regresión\n",
        "try:\n",
        "    data_train = pd.read_parquet('data_train.parquet')\n",
        "    data_test = pd.read_parquet('data_test.parquet')\n",
        "except:\n",
        "    X_range = np.linspace(-5, 5, 1000)\n",
        "    y_function = X_range ** 3 * 0.001 + np.sin(X_range * 2) * 0.5 + np.random.normal(0, 0.1, 1000)\n",
        "    split = int(0.8 * len(X_range))\n",
        "    data_train = pd.DataFrame({'X': X_range[:split], 'Y': y_function[:split]})\n",
        "    data_test = pd.DataFrame({'X': X_range[split:], 'Y': y_function[split:]})\n",
        "X_train = data_train[['X']].values\n",
        "y_train = data_train[['Y']].values\n",
        "X_test = data_test[['X']].values\n",
        "y_test = data_test[['Y']].values\n",
        "X_mean, X_std = X_train.mean(), X_train.std()\n",
        "y_mean, y_std = y_train.mean(), y_train.std()\n",
        "X_train_norm = (X_train - X_mean) / X_std\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "y_train_norm = (y_train - y_mean) / y_std\n",
        "y_test_norm = (y_test - y_mean) / y_std\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparámetros regresión\n",
        "configs_regression = [\n",
        "    {'hidden_size': 8, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.005, 'epochs': 4000, 'activation': 'relu'},\n",
        "    {'hidden_size': 32, 'learning_rate': 0.003, 'epochs': 5000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 24, 'learning_rate': 0.008, 'epochs': 4000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 12, 'learning_rate': 0.02, 'epochs': 2500, 'activation': 'sigmoid'}\n",
        "]\n",
        "mejor_r2 = -float('inf')\n",
        "mejor_modelo_reg = None\n",
        "for config in configs_regression:\n",
        "    mlp = MultiLayerPerceptron(input_size=1, output_size=1, task='regression', **config)\n",
        "    mlp.fit(X_train_norm, y_train_norm, verbose=False)\n",
        "    y_pred_norm = mlp.predict(X_test_norm)\n",
        "    y_pred = y_pred_norm * y_std + y_mean\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    if r2 > mejor_r2:\n",
        "        mejor_r2 = r2\n",
        "        mejor_modelo_reg = mlp\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización regresión\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mejor_modelo_reg.train_errors, linewidth=2)\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Curva de Aprendizaje - Regresión')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(X_train, y_train, '.', label='Train')\n",
        "plt.plot(X_test, y_test, '.', label='Test')\n",
        "plt.plot(X_test, mejor_modelo_reg.predict(X_test_norm) * y_std + y_mean, linewidth=2, label='Predicción MLP')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Regresión: Predicción del MLP')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {}
}
