{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Práctica 0: Implementación de Perceptrón Multicapa desde Cero\n",
        "\n",
        "Implementación de un perceptrón multicapa (MLP) sin usar frameworks de deep learning, solo NumPy.\n",
        "\n",
        "**Objetivos:**\n",
        "- Implementar forward pass y backpropagation manualmente\n",
        "- Probar con XOR (problema no linealmente separable)\n",
        "- Aplicar a problema de regresión\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración inicial\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "print(\"Configuración inicial completada\")"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementación del Perceptrón Multicapa\n",
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                 learning_rate=0.01, epochs=5000, activation='relu', task='classification'):\n",
        "        # Inicialización Xavier\n",
        "        xavier_std = np.sqrt(2.0 / (input_size + hidden_size))\n",
        "        self.W1 = np.random.normal(0, xavier_std, (input_size, hidden_size))\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        \n",
        "        xavier_std2 = np.sqrt(2.0 / (hidden_size + output_size))\n",
        "        self.W2 = np.random.normal(0, xavier_std2, (hidden_size, output_size))\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.activation = activation\n",
        "        self.task = task\n",
        "        \n",
        "        self.train_errors = []\n",
        "        self.validation_errors = []\n",
        "    \n",
        "    def _activation_function(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "        elif self.activation == 'tanh':\n",
        "            return np.tanh(x)\n",
        "    \n",
        "    def _activation_derivative(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return (x > 0).astype(float)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return x * (1 - x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return 1 - x ** 2\n",
        "    \n",
        "    def _output_function(self, x):\n",
        "        if self.task == 'classification':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "    def _output_derivative(self, x):\n",
        "        if self.task == 'classification':\n",
        "            return x * (1 - x)\n",
        "        else:\n",
        "            return np.ones_like(x)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # Capa oculta\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self._activation_function(self.z1)\n",
        "        \n",
        "        # Capa de salida\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.output = self._output_function(self.z2)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Error capa de salida\n",
        "        error = y - self.output\n",
        "        d_output = error * self._output_derivative(self.output)\n",
        "        \n",
        "        # Error capa oculta\n",
        "        error_hidden = d_output.dot(self.W2.T)\n",
        "        d_hidden = error_hidden * self._activation_derivative(self.a1)\n",
        "        \n",
        "        # Actualizar pesos\n",
        "        self.W2 += (self.a1.T.dot(d_output) / m) * self.learning_rate\n",
        "        self.b2 += np.mean(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.W1 += (X.T.dot(d_hidden) / m) * self.learning_rate\n",
        "        self.b1 += np.mean(d_hidden, axis=0, keepdims=True) * self.learning_rate\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None, verbose=True):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Registrar errores\n",
        "            mse_train = np.mean((y - self.output) ** 2)\n",
        "            self.train_errors.append(mse_train)\n",
        "            \n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self.forward(X_val)\n",
        "                mse_val = np.mean((y_val - val_pred) ** 2)\n",
        "                self.validation_errors.append(mse_val)\n",
        "            \n",
        "            # Mostrar progreso\n",
        "            if verbose and (epoch % (self.epochs // 10) == 0 or epoch == self.epochs - 1):\n",
        "                if X_val is not None:\n",
        "                    print(f\"Época {epoch:4d}: Train MSE = {mse_train:.6f}, Val MSE = {mse_val:.6f}\")\n",
        "                else:\n",
        "                    print(f\"Época {epoch:4d}: MSE = {mse_train:.6f}\")\n",
        "        \n",
        "        return time.time() - start_time\n",
        "    \n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        if self.task == 'classification':\n",
        "            return (output > 0.5).astype(int)\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "print(\"Clase MultiLayerPerceptron definida\")"
      ],
      "metadata": {
        "id": "mlp_class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problema XOR - verificación del MLP\n",
        "print(\"=== PROBLEMA XOR ===")\n",
        "\n",
        "# Datos XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "print(\"Datos XOR:\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"  {X_xor[i]} -> {y_xor[i][0]}\")\n",
        "\n",
        "# Configuraciones a probar\n",
        "configs_xor = [\n",
        "    {'hidden_size': 2, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 4, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 8, 'learning_rate': 0.05, 'epochs': 2000, 'activation': 'relu'},\n",
        "    {'hidden_size': 10, 'learning_rate': 0.01, 'epochs': 5000, 'activation': 'tanh'}\n",
        "]\n",
        "\n",
        "print(\"\\nEntrenando diferentes configuraciones:\")\n",
        "\n",
        "mejor_accuracy = 0\n",
        "mejor_modelo_xor = None\n",
        "\n",
        "for i, config in enumerate(configs_xor):\n",
        "    print(f\"Config {i+1}: {config}\")\n",
        "    \n",
        "    mlp = MultiLayerPerceptron(\n",
        "        input_size=2,\n",
        "        output_size=1,\n",
        "        task='classification',\n",
        "        **config\n",
        "    )\n",
        "    \n",
        "    tiempo = mlp.fit(X_xor, y_xor, verbose=False)\n",
        "    predicciones = mlp.predict(X_xor)\n",
        "    accuracy = np.mean(y_xor == predicciones)\n",
        "    \n",
        "    print(f\"  Tiempo: {tiempo:.3f}s | Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Predicciones: {predicciones.flatten()}\")\n",
        "    \n",
        "    if accuracy > mejor_accuracy:\n",
        "        mejor_accuracy = accuracy\n",
        "        mejor_modelo_xor = mlp\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(f\"Mejor accuracy XOR: {mejor_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "xor_problem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar frontera de decisión XOR\n",
        "def plot_decision_boundary(model, X, y, title=\"Frontera de Decisión\"):\n",
        "    h = 0.01\n",
        "    x_min, x_max = -0.1, 1.1\n",
        "    y_min, y_max = -0.1, 1.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.forward(mesh_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdBu')\n",
        "    plt.colorbar(label='Salida')\n",
        "    \n",
        "    colors = ['red' if label == 0 else 'blue' for label in y.flatten()]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "    \n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Curva de aprendizaje\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mejor_modelo_xor.train_errors, linewidth=2)\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Curva de Aprendizaje - XOR')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plot_decision_boundary(mejor_modelo_xor, X_xor, y_xor, \"Frontera de Decisión XOR\")"
      ],
      "metadata": {
        "id": "xor_visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problema de Regresión\n",
        "print(\"=== PROBLEMA DE REGRESIÓN ===")\n",
        "\n",
        "# Cargar o generar datos\n",
        "try:\n",
        "    data_train = pd.read_parquet('data_train.parquet')\n",
        "    data_test = pd.read_parquet('data_test.parquet')\n",
        "    print(\"Datos cargados desde archivos parquet\")\n",
        "except:\n",
        "    # Generar datos sintéticos\n",
        "    print(\"Generando datos sintéticos...\")\n",
        "    X_range = np.linspace(-5, 5, 1000)\n",
        "    y_function = X_range ** 3 * 0.001 + np.sin(X_range * 2) * 0.5 + np.random.normal(0, 0.1, 1000)\n",
        "    \n",
        "    split = int(0.8 * len(X_range))\n",
        "    data_train = pd.DataFrame({'X': X_range[:split], 'Y': y_function[:split]})\n",
        "    data_test = pd.DataFrame({'X': X_range[split:], 'Y': y_function[split:]})\n",
        "\n",
        "X_train = data_train[['X']].values\n",
        "y_train = data_train[['Y']].values\n",
        "X_test = data_test[['X']].values\n",
        "y_test = data_test[['Y']].values\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]} muestras\")\n",
        "print(f\"Test: {X_test.shape[0]} muestras\")\n",
        "\n",
        "# Normalización\n",
        "X_mean, X_std = X_train.mean(), X_train.std()\n",
        "y_mean, y_std = y_train.mean(), y_train.std()\n",
        "\n",
        "X_train_norm = (X_train - X_mean) / X_std\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "y_train_norm = (y_train - y_mean) / y_std\n",
        "y_test_norm = (y_test - y_mean) / y_std\n",
        "\n",
        "print(f\"Normalización aplicada: X_mean={X_mean:.3f}, X_std={X_std:.3f}\")"
      ],
      "metadata": {
        "id": "regression_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Búsqueda de hiperparámetros para regresión\n",
        "configs_regression = [\n",
        "    {'hidden_size': 8, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.005, 'epochs': 4000, 'activation': 'relu'},\n",
        "    {'hidden_size': 32, 'learning_rate': 0.003, 'epochs': 5000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 24, 'learning_rate': 0.008, 'epochs': 4000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 12, 'learning_rate': 0.02, 'epochs': 2500, 'activation': 'sigmoid'}\n",
        "]\n",
        "\n",
        "print(\"Probando configuraciones para regresión:\")\n",
        "\n",
        "resultados = []\n",
        "mejor_r2 = -float('inf')\n",
        "mejor_modelo_reg = None\n",
        "\n",
        "for i, config in enumerate(configs_regression):\n",
        "    print(f\"Config {i+1}: {config}\")\n",
        "    \n",
        "    mlp = MultiLayerPerceptron(\n",
        "        input_size=1,\n",
        "        output_size=1,\n",
        "        task='regression',\n",
        "        **config\n",
        "    )\n",
        "    \n",
        "    tiempo = mlp.fit(X_train_norm, y_train_norm, verbose=False)\n",
        "    \n",
        "    # Evaluar\n",
        "    y_pred_norm = mlp.predict(X_test_norm)\n",
        "    y_pred = y_pred_norm * y_std + y_mean  # Desnormalizar\n",
        "    \n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    resultados.append({\n",
        "        'config': config,\n",
        "        'tiempo': tiempo,\n",
        "        'mse': mse,\n",
        "        'r2': r2\n",
        "    })\n",
        "    \n",
        "    print(f\"  Tiempo: {tiempo:.3f}s | MSE: {mse:.6f} | R2: {r2:.6f}\")\n",
        "    \n",
        "    if r2 > mejor_r2:\n",
        "        mejor_r2 = r2\n",
        "        mejor_modelo_reg = mlp\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(f\"Mejor R2 Score: {mejor_r2:.6f}\")"
      ],
      "metadata": {
        "id": "regression_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de resultados de regresión\n",
        "\n",
        "# Curva de aprendizaje\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mejor_modelo_reg.train_errors, linewidth=2)\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Curva de Aprendizaje - Regresión')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Predicciones vs reales\n",
        "y_pred_norm = mejor_modelo_reg.predict(X_test_norm)\n",
        "y_pred = y_pred_norm * y_std + y_mean\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Scatter plot\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, s=20)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Valores Reales')\n",
        "plt.ylabel('Predicciones')\n",
        "plt.title('Predicciones vs Reales')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Función vs aproximación\n",
        "plt.subplot(1, 3, 2)\n",
        "indices = np.argsort(X_test.flatten())\n",
        "plt.plot(X_test[indices], y_test[indices], 'b-', label='Real', linewidth=2)\n",
        "plt.plot(X_test[indices], y_pred[indices], 'r--', label='Predicción', linewidth=2)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Función vs Aproximación')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuos\n",
        "plt.subplot(1, 3, 3)\n",
        "residuos = y_test - y_pred\n",
        "plt.hist(residuos, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Residuos')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de Residuos')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Métricas finales\n",
        "mse_final = mean_squared_error(y_test, y_pred)\n",
        "mae_final = mean_absolute_error(y_test, y_pred)\n",
        "r2_final = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Métricas finales:\")\n",
        "print(f\"  MSE: {mse_final:.6f}\")\n",
        "print(f\"  MAE: {mae_final:.6f}\")\n",
        "print(f\"  R²: {r2_final:.6f}\")"
      ],
      "metadata": {
        "id": "regression_visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabla comparativa de todas las configuraciones\n",
        "df_resultados = pd.DataFrame([\n",
        "    {\n",
        "        'Config': i+1,\n",
        "        'Hidden Size': r['config']['hidden_size'],\n",
        "        'Learning Rate': r['config']['learning_rate'],\n",
        "        'Épocas': r['config']['epochs'],\n",
        "        'Activación': r['config']['activation'],\n",
        "        'Tiempo (s)': f\"{r['tiempo']:.3f}\",\n",
        "        'MSE': f\"{r['mse']:.6f}\",\n",
        "        'R² Score': f\"{r['r2']:.6f}\"\n",
        "    }\n",
        "    for i, r in enumerate(resultados)\n",
        "])\n",
        "\n",
        "print(\"\\nComparación de todas las configuraciones:\")\n",
        "print(df_resultados.to_string(index=False))\n",
        "\n",
        "# Visualización comparativa\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "configs_num = range(1, len(resultados) + 1)\n",
        "r2_scores = [r['r2'] for r in resultados]\n",
        "mse_scores = [r['mse'] for r in resultados]\n",
        "tiempos = [r['tiempo'] for r in resultados]\n",
        "\n",
        "axes[0].bar(configs_num, r2_scores, color='skyblue', alpha=0.7)\n",
        "axes[0].set_title('R² Score por Configuración')\n",
        "axes[0].set_xlabel('Configuración')\n",
        "axes[0].set_ylabel('R² Score')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(configs_num, mse_scores, color='lightcoral', alpha=0.7)\n",
        "axes[1].set_title('MSE por Configuración')\n",
        "axes[1].set_xlabel('Configuración')\n",
        "axes[1].set_ylabel('MSE')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].bar(configs_num, tiempos, color='lightgreen', alpha=0.7)\n",
        "axes[2].set_title('Tiempo de Entrenamiento')\n",
        "axes[2].set_xlabel('Configuración')\n",
        "axes[2].set_ylabel('Tiempo (s)')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "comparison_table"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}