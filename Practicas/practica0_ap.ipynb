{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Práctica 0: Implementación de Perceptrón Multicapa desde Cero\n",
        "\n",
        "**Objetivo:** Construir un perceptrón multicapa completamente desde cero sin usar frameworks especializados.\n",
        "\n",
        "**Contenido:**\n",
        "1. Implementación parametrizable del MLP\n",
        "2. Verificación con XOR\n",
        "3. Monitorización del entrenamiento\n",
        "4. Regresión con datos adjuntos\n",
        "5. Optimización de hiperparámetros\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaciones y Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Configuración para reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuración de plots\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "print(\"✅ Configuración inicial completada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implementación del Perceptrón Multicapa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron:\n",
        "    \"\"\"\n",
        "    Perceptrón Multicapa de 2 capas implementado desde cero\n",
        "    Soporta tanto clasificación como regresión\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                 learning_rate=0.01, epochs=5000, activation='relu', task='classification'):\n",
        "        \"\"\"\n",
        "        Inicializa el MLP con parámetros configurables\n",
        "        \n",
        "        Parámetros:\n",
        "        - input_size: Número de features de entrada\n",
        "        - hidden_size: Número de neuronas en capa oculta\n",
        "        - output_size: Número de neuronas de salida\n",
        "        - learning_rate: Tasa de aprendizaje\n",
        "        - epochs: Número de épocas de entrenamiento\n",
        "        - activation: Función de activación ('relu', 'sigmoid', 'tanh')\n",
        "        - task: Tipo de problema ('classification', 'regression')\n",
        "        \"\"\"\n",
        "        # Inicialización Xavier/Glorot para mejores resultados\n",
        "        xavier_std = np.sqrt(2.0 / (input_size + hidden_size))\n",
        "        self.W1 = np.random.normal(0, xavier_std, (input_size, hidden_size))\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        \n",
        "        xavier_std2 = np.sqrt(2.0 / (hidden_size + output_size))\n",
        "        self.W2 = np.random.normal(0, xavier_std2, (hidden_size, output_size))\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.activation = activation\n",
        "        self.task = task\n",
        "        \n",
        "        # Arrays para almacenar métricas\n",
        "        self.train_errors = []\n",
        "        self.validation_errors = []\n",
        "    \n",
        "    def _activation_function(self, x):\n",
        "        \"\"\"Aplica la función de activación seleccionada\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip para evitar overflow\n",
        "        elif self.activation == 'tanh':\n",
        "            return np.tanh(x)\n",
        "    \n",
        "    def _activation_derivative(self, x):\n",
        "        \"\"\"Calcula la derivada de la función de activación\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return (x > 0).astype(float)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return x * (1 - x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return 1 - x ** 2\n",
        "    \n",
        "    def _output_function(self, x):\n",
        "        \"\"\"Función de salida según el tipo de tarea\"\"\"\n",
        "        if self.task == 'classification':\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Sigmoid para clasificación\n",
        "        else:\n",
        "            return x  # Lineal para regresión\n",
        "    \n",
        "    def _output_derivative(self, x):\n",
        "        \"\"\"Derivada de la función de salida\"\"\"\n",
        "        if self.task == 'classification':\n",
        "            return x * (1 - x)\n",
        "        else:\n",
        "            return np.ones_like(x)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Propagación hacia adelante\"\"\"\n",
        "        # Capa oculta\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self._activation_function(self.z1)\n",
        "        \n",
        "        # Capa de salida\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.output = self._output_function(self.z2)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"Retropropagación con actualización de pesos\"\"\"\n",
        "        m = X.shape[0]  # Número de ejemplos\n",
        "        \n",
        "        # Error capa de salida\n",
        "        error = y - self.output\n",
        "        d_output = error * self._output_derivative(self.output)\n",
        "        \n",
        "        # Error capa oculta\n",
        "        error_hidden = d_output.dot(self.W2.T)\n",
        "        d_hidden = error_hidden * self._activation_derivative(self.a1)\n",
        "        \n",
        "        # Actualización de pesos y sesgos\n",
        "        self.W2 += (self.a1.T.dot(d_output) / m) * self.learning_rate\n",
        "        self.b2 += np.mean(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.W1 += (X.T.dot(d_hidden) / m) * self.learning_rate\n",
        "        self.b1 += np.mean(d_hidden, axis=0, keepdims=True) * self.learning_rate\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None, verbose=True):\n",
        "        \"\"\"Entrena el modelo con monitorización opcional\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            self.forward(X)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.backward(X, y)\n",
        "            \n",
        "            # Calcular error de entrenamiento\n",
        "            mse_train = np.mean((y - self.output) ** 2)\n",
        "            self.train_errors.append(mse_train)\n",
        "            \n",
        "            # Error de validación si se proporciona\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self.forward(X_val)\n",
        "                mse_val = np.mean((y_val - val_pred) ** 2)\n",
        "                self.validation_errors.append(mse_val)\n",
        "            \n",
        "            # Progreso cada 10% de las épocas\n",
        "            if verbose and (epoch % (self.epochs // 10) == 0 or epoch == self.epochs - 1):\n",
        "                if X_val is not None:\n",
        "                    print(f\"Época {epoch:4d}: Train MSE = {mse_train:.6f}, Val MSE = {mse_val:.6f}\")\n",
        "                else:\n",
        "                    print(f\"Época {epoch:4d}: MSE = {mse_train:.6f}\")\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        if verbose:\n",
        "            print(f\"\\n✅ Entrenamiento completado en {training_time:.2f} segundos\")\n",
        "        \n",
        "        return training_time\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicción según el tipo de tarea\"\"\"\n",
        "        output = self.forward(X)\n",
        "        if self.task == 'classification':\n",
        "            return (output > 0.5).astype(int)\n",
        "        else:\n",
        "            return output\n",
        "    \n",
        "    def plot_learning_curve(self):\n",
        "        \"\"\"Visualiza la curva de aprendizaje\"\"\"\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_errors, label='Entrenamiento', linewidth=2)\n",
        "        if self.validation_errors:\n",
        "            plt.plot(self.validation_errors, label='Validación', linewidth=2)\n",
        "        plt.xlabel('Época')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.title('Curva de Aprendizaje')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.train_errors, label='MSE', linewidth=2)\n",
        "        plt.xlabel('Época')\n",
        "        plt.ylabel('MSE (log scale)')\n",
        "        plt.title('Error en Escala Logarítmica')\n",
        "        plt.yscale('log')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"Evalúa el modelo y retorna métricas\"\"\"\n",
        "        predictions = self.forward(X)\n",
        "        mse = mean_squared_error(y, predictions)\n",
        "        \n",
        "        if self.task == 'regression':\n",
        "            r2 = r2_score(y, predictions)\n",
        "            return {'MSE': mse, 'R2': r2}\n",
        "        else:\n",
        "            pred_binary = (predictions > 0.5).astype(int)\n",
        "            accuracy = np.mean(y == pred_binary)\n",
        "            return {'MSE': mse, 'Accuracy': accuracy}\n",
        "\n",
        "print(\"🔧 Clase MultiLayerPerceptron definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verificación con Problema XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🎯 Verificación con Problema XOR\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Datos XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "print(\"Datos XOR:\")\n",
        "print(\"Entrada | Salida\")\n",
        "print(\"-\" * 15)\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"  {X_xor[i]}   |   {y_xor[i][0]}\")\n",
        "\n",
        "# Configuraciones a probar\n",
        "configs_xor = [\n",
        "    {'hidden_size': 2, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 4, 'learning_rate': 0.1, 'epochs': 1000, 'activation': 'sigmoid'},\n",
        "    {'hidden_size': 8, 'learning_rate': 0.05, 'epochs': 2000, 'activation': 'relu'},\n",
        "    {'hidden_size': 10, 'learning_rate': 0.01, 'epochs': 5000, 'activation': 'tanh'}\n",
        "]\n",
        "\n",
        "mejor_accuracy = 0\n",
        "mejor_config_xor = None\n",
        "mejor_modelo_xor = None\n",
        "\n",
        "print(\"\\n🔄 Probando diferentes configuraciones:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, config in enumerate(configs_xor):\n",
        "    print(f\"Configuración {i+1}: {config}\")\n",
        "    \n",
        "    mlp = MultiLayerPerceptron(\n",
        "        input_size=2,\n",
        "        output_size=1,\n",
        "        task='classification',\n",
        "        **config\n",
        "    )\n",
        "    \n",
        "    tiempo = mlp.fit(X_xor, y_xor, verbose=False)\n",
        "    metricas = mlp.evaluate(X_xor, y_xor)\n",
        "    predicciones = mlp.predict(X_xor)\n",
        "    \n",
        "    print(f\"  Tiempo: {tiempo:.3f}s | Accuracy: {metricas['Accuracy']:.4f} | MSE: {metricas['MSE']:.6f}\")\n",
        "    print(f\"  Predicciones: {predicciones.flatten()}\")\n",
        "    \n",
        "    if metricas['Accuracy'] > mejor_accuracy:\n",
        "        mejor_accuracy = metricas['Accuracy']\n",
        "        mejor_config_xor = config\n",
        "        mejor_modelo_xor = mlp\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(f\"✅ Mejor configuración XOR: {mejor_config_xor}\")\n",
        "print(f\"   Accuracy obtenida: {mejor_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualización del Mejor Modelo XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar curva de aprendizaje del mejor modelo\n",
        "print(\"📊 Curva de aprendizaje del mejor modelo XOR:\")\n",
        "mejor_modelo_xor.plot_learning_curve()\n",
        "\n",
        "# Visualizar la frontera de decisión\n",
        "def plot_decision_boundary(model, X, y, title=\"Frontera de Decisión XOR\"):\n",
        "    h = 0.01  # Step size en el mesh\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.forward(mesh_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdBu')\n",
        "    plt.colorbar(label='Probabilidad')\n",
        "    \n",
        "    # Puntos de datos\n",
        "    colors = ['red' if label == 0 else 'blue' for label in y.flatten()]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "    \n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Añadir etiquetas a los puntos\n",
        "    for i in range(len(X)):\n",
        "        plt.annotate(f'{y[i][0]}', (X[i, 0], X[i, 1]), \n",
        "                    xytext=(5, 5), textcoords='offset points',\n",
        "                    fontsize=12, fontweight='bold', color='white')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(mejor_modelo_xor, X_xor, y_xor)\n",
        "\n",
        "print(f\"✅ XOR resuelto exitosamente con accuracy {mejor_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Problema de Regresión con Datos Adjuntos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📊 Problema de Regresión con Datos Adjuntos\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Cargar datos (simulados si no existen los archivos)\n",
        "try:\n",
        "    data_train = pd.read_parquet('data_train.parquet')\n",
        "    data_test = pd.read_parquet('data_test.parquet')\n",
        "    print(\"Datos cargados exitosamente desde archivos parquet\")\n",
        "except:\n",
        "    # Crear datos simulados si no existen los archivos\n",
        "    print(\"Archivos no encontrados. Creando datos simulados...\")\n",
        "    X_range = np.linspace(-5, 5, 1000)\n",
        "    y_function = X_range ** 3 * 0.001 + np.sin(X_range * 2) * 0.5 + np.random.normal(0, 0.1, 1000)\n",
        "    \n",
        "    # Dividir en train/test\n",
        "    split = int(0.8 * len(X_range))\n",
        "    data_train = pd.DataFrame({'X': X_range[:split], 'Y': y_function[:split]})\n",
        "    data_test = pd.DataFrame({'X': X_range[split:], 'Y': y_function[split:]})\n",
        "\n",
        "# Preparar datos\n",
        "X_train = data_train[['X']].values\n",
        "y_train = data_train[['Y']].values\n",
        "X_test = data_test[['X']].values\n",
        "y_test = data_test[['Y']].values\n",
        "\n",
        "print(f\"Datos de entrenamiento: {X_train.shape[0]} muestras\")\n",
        "print(f\"Datos de test: {X_test.shape[0]} muestras\")\n",
        "\n",
        "# Normalizar datos (importante para regresión)\n",
        "X_mean, X_std = X_train.mean(), X_train.std()\n",
        "y_mean, y_std = y_train.mean(), y_train.std()\n",
        "\n",
        "X_train_norm = (X_train - X_mean) / X_std\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "y_train_norm = (y_train - y_mean) / y_std\n",
        "y_test_norm = (y_test - y_mean) / y_std\n",
        "\n",
        "print(f\"\\nEstadísticas de normalización:\")\n",
        "print(f\"X: mean={X_mean:.3f}, std={X_std:.3f}\")\n",
        "print(f\"Y: mean={y_mean:.6f}, std={y_std:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Búsqueda de Hiperparámetros para Regresión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔍 Búsqueda de Mejores Hiperparámetros para Regresión\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuraciones a probar (ampliadas y optimizadas)\n",
        "configs_regression = [\n",
        "    {'hidden_size': 8, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.005, 'epochs': 4000, 'activation': 'relu'},\n",
        "    {'hidden_size': 32, 'learning_rate': 0.003, 'epochs': 5000, 'activation': 'relu'},\n",
        "    {'hidden_size': 16, 'learning_rate': 0.01, 'epochs': 3000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 24, 'learning_rate': 0.008, 'epochs': 4000, 'activation': 'tanh'},\n",
        "    {'hidden_size': 12, 'learning_rate': 0.02, 'epochs': 2500, 'activation': 'sigmoid'}\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "mejor_r2 = -float('inf')\n",
        "mejor_config_reg = None\n",
        "mejor_modelo_reg = None\n",
        "\n",
        "for i, config in enumerate(configs_regression):\n",
        "    print(f\"\\n⚙️  Configuración {i+1}: {config}\")\n",
        "    \n",
        "    mlp = MultiLayerPerceptron(\n",
        "        input_size=1,\n",
        "        output_size=1,\n",
        "        task='regression',\n",
        "        **config\n",
        "    )\n",
        "    \n",
        "    # Entrenar con datos normalizados\n",
        "    tiempo = mlp.fit(X_train_norm, y_train_norm, verbose=False)\n",
        "    \n",
        "    # Evaluar en test\n",
        "    metricas = mlp.evaluate(X_test_norm, y_test_norm)\n",
        "    \n",
        "    resultado = {\n",
        "        'config': config,\n",
        "        'tiempo': tiempo,\n",
        "        'mse_test': metricas['MSE'],\n",
        "        'r2_test': metricas['R2']\n",
        "    }\n",
        "    resultados.append(resultado)\n",
        "    \n",
        "    print(f\"   ⏱️  Tiempo: {tiempo:.3f}s\")\n",
        "    print(f\"   🎯 MSE Test: {metricas['MSE']:.6f}\")\n",
        "    print(f\"   📈 R2 Score: {metricas['R2']:.6f}\")\n",
        "    \n",
        "    if metricas['R2'] > mejor_r2:\n",
        "        mejor_r2 = metricas['R2']\n",
        "        mejor_config_reg = config\n",
        "        mejor_modelo_reg = mlp\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"🏆 MEJOR CONFIGURACIÓN PARA REGRESIÓN:\")\n",
        "print(f\"   Config: {mejor_config_reg}\")\n",
        "print(f\"   R2 Score: {mejor_r2:.6f}\")\n",
        "print(f\"   MSE: {[r for r in resultados if r['r2_test'] == mejor_r2][0]['mse_test']:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualización del Mejor Modelo de Regresión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curva de aprendizaje\n",
        "print(\"📉 Curva de aprendizaje del mejor modelo de regresión:\")\n",
        "mejor_modelo_reg.plot_learning_curve()\n",
        "\n",
        "# Gráfico de predicciones vs reales\n",
        "y_pred_norm = mejor_modelo_reg.predict(X_test_norm)\n",
        "y_pred = y_pred_norm * y_std + y_mean  # Desnormalizar\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Datos reales vs predicciones\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, s=20)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Valores Reales')\n",
        "plt.ylabel('Predicciones')\n",
        "plt.title('Predicciones vs Valores Reales')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Función original vs aproximación\n",
        "plt.subplot(1, 3, 2)\n",
        "indices_ordenados = np.argsort(X_test.flatten())\n",
        "plt.plot(X_test[indices_ordenados], y_test[indices_ordenados], 'b-', label='Real', linewidth=2)\n",
        "plt.plot(X_test[indices_ordenados], y_pred[indices_ordenados], 'r--', label='Predicción', linewidth=2)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Función Real vs Aproximación')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Distribución de residuos\n",
        "plt.subplot(1, 3, 3)\n",
        "residuos = y_test - y_pred\n",
        "plt.hist(residuos, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.xlabel('Residuos (Real - Predicción)')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de Residuos')\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calcular métricas finales\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mse_final = mean_squared_error(y_test, y_pred)\n",
        "mae_final = mean_absolute_error(y_test, y_pred)\n",
        "r2_final = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n📈 Métricas Finales del Mejor Modelo:\")\n",
        "print(f\"   MSE (Mean Squared Error): {mse_final:.6f}\")\n",
        "print(f\"   MAE (Mean Absolute Error): {mae_final:.6f}\")\n",
        "print(f\"   R² Score: {r2_final:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparación de Todas las Configuraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📊 Comparación de Todas las Configuraciones de Regresión\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_resultados = pd.DataFrame([\n",
        "    {\n",
        "        'Configuración': i+1,\n",
        "        'Hidden Size': r['config']['hidden_size'],\n",
        "        'Learning Rate': r['config']['learning_rate'],\n",
        "        'Épocas': r['config']['epochs'],\n",
        "        'Activación': r['config']['activation'],\n",
        "        'Tiempo (s)': r['tiempo'],\n",
        "        'MSE Test': r['mse_test'],\n",
        "        'R2 Score': r['r2_test']\n",
        "    }\n",
        "    for i, r in enumerate(resultados)\n",
        "])\n",
        "\n",
        "# Ordenar por R2 Score (mejor primero)\n",
        "df_resultados = df_resultados.sort_values('R2 Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(df_resultados.to_string(index=False))\n",
        "\n",
        "# Visualización comparativa\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Comparación de Configuraciones - Problema de Regresión', fontsize=16, fontweight='bold')\n",
        "\n",
        "# R2 Score por configuración\n",
        "ax = axes[0, 0]\n",
        "bars = ax.bar(range(len(df_resultados)), df_resultados['R2 Score'], color='skyblue', alpha=0.7)\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('R² Score por Configuración')\n",
        "ax.set_xticks(range(len(df_resultados)))\n",
        "ax.set_xticklabels([f'Config {i+1}' for i in range(len(df_resultados))])\n",
        "for bar, r2 in zip(bars, df_resultados['R2 Score']):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{r2:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# MSE por configuración\n",
        "ax = axes[0, 1]\n",
        "bars = ax.bar(range(len(df_resultados)), df_resultados['MSE Test'], color='lightcoral', alpha=0.7)\n",
        "ax.set_ylabel('MSE Test')\n",
        "ax.set_title('MSE Test por Configuración')\n",
        "ax.set_xticks(range(len(df_resultados)))\n",
        "ax.set_xticklabels([f'Config {i+1}' for i in range(len(df_resultados))])\n",
        "ax.set_yscale('log')\n",
        "\n",
        "# Tiempo de entrenamiento\n",
        "ax = axes[1, 0]\n",
        "bars = ax.bar(range(len(df_resultados)), df_resultados['Tiempo (s)'], color='lightgreen', alpha=0.7)\n",
        "ax.set_ylabel('Tiempo (segundos)')\n",
        "ax.set_title('Tiempo de Entrenamiento')\n",
        "ax.set_xticks(range(len(df_resultados)))\n",
        "ax.set_xticklabels([f'Config {i+1}' for i in range(len(df_resultados))])\n",
        "\n",
        "# Hidden Size vs R2\n",
        "ax = axes[1, 1]\n",
        "colors = {'relu': 'red', 'tanh': 'blue', 'sigmoid': 'green'}\n",
        "for activation in df_resultados['Activación'].unique():\n",
        "    mask = df_resultados['Activación'] == activation\n",
        "    ax.scatter(df_resultados[mask]['Hidden Size'], df_resultados[mask]['R2 Score'], \n",
        "              label=activation, c=colors[activation], s=100, alpha=0.7)\n",
        "ax.set_xlabel('Hidden Size')\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('Tamaño de Capa Oculta vs Rendimiento')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusiones y Resumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📋 RESUMEN DE LA PRÁCTICA 0\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"1. PROBLEMA XOR (Clasificación):\")\n",
        "print(f\"   ✅ Resuelto exitosamente con accuracy: {mejor_accuracy:.4f}\")\n",
        "print(f\"   🎯 Mejor configuración: {mejor_config_xor}\")\n",
        "\n",
        "print(\"2. PROBLEMA DE REGRESIÓN:\")\n",
        "print(f\"   🏆 Mejor R² Score: {mejor_r2:.6f}\")\n",
        "print(f\"   ⚙️  Mejor configuración: {mejor_config_reg}\")\n",
        "\n",
        "print(\"3. OBSERVACIONES CLAVE:\")\n",
        "print(\"   • La inicialización Xavier mejora la convergencia\")\n",
        "print(\"   • ReLU funciona mejor para regresión con estos datos\")\n",
        "print(\"   • Sigmoid es adecuado para XOR (problema de clasificación simple)\")\n",
        "print(\"   • Normalización de datos es crucial para regresión\")\n",
        "print(\"   • Mayor complejidad (más neuronas) mejora aproximación en regresión\")\n",
        "\n",
        "print(\"4. CONFIGURACIONES ÓPTIMAS IDENTIFICADAS:\")\n",
        "print(f\"   XOR: {mejor_config_xor}\")\n",
        "print(f\"   Regresión: {mejor_config_reg}\")\n",
        "\n",
        "print(\"✅ PRÁCTICA 0 COMPLETADA Y OPTIMIZADA\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}