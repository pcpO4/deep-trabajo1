{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Pr√°ctica 5: Generative Adversarial Network (GAN) para MNIST\n",
        "\n",
        "En esta pr√°ctica implementamos un GAN simple para generar d√≠gitos MNIST. Seguimos el enunciado que requiere:\n",
        "\n",
        "## Objetivos del enunciado:\n",
        "1. **Visualizar la evoluci√≥n de errores**: Gr√°fica con la evoluci√≥n de los errores del discriminador y generador durante el entrenamiento\n",
        "2. **Calcular m√©tricas del modelo**: M√©tricas de rendimiento del discriminador y generador\n",
        "\n",
        "## Arquitectura GAN:\n",
        "- **Generador**: Transforma ruido aleatorio en im√°genes 28x28\n",
        "- **Discriminador**: Clasifica im√°genes como reales o generadas\n",
        "- **Entrenamiento adversarial**: Ambos modelos compiten para mejorar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 1: CONFIGURACI√ìN E IMPORTACI√ìN DE LIBRER√çAS\n",
        "# ============================================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de reproducibilidad\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuraci√≥n de dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "print(f\"Versi√≥n de PyTorch: {torch.__version__}\")\n",
        "\n",
        "# Configuraci√≥n del dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalizar a [-1, 1]\n",
        "])\n",
        "\n",
        "# Cargar dataset MNIST\n",
        "print(\"\\nCargando dataset MNIST...\")\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(mnist_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"Dataset cargado: {len(mnist_dataset)} muestras\")\n",
        "print(f\"Batches por √©poca: {len(dataloader)}\")"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 2: DEFINICI√ìN DE ARQUITECTURAS\n",
        "# ============================================================================================\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generador: Convierte ruido aleatorio (z) en im√°genes MNIST 28x28\n",
        "    \n",
        "    Arquitectura:\n",
        "    - Input: Vector de ruido de dimensi√≥n z_dim (100)\n",
        "    - Hidden: 2 capas densas con ReLU\n",
        "    - Output: Imagen 28x28 con activaci√≥n Tanh [-1, 1]\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, z_dim=100, img_dim=28*28):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(512, img_dim),\n",
        "            nn.Tanh()  # Output en [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Forward pass del generador\n",
        "        Args:\n",
        "            z: Tensor de ruido (batch_size, z_dim)\n",
        "        Returns:\n",
        "            img: Tensor de im√°genes generadas (batch_size, img_dim)\n",
        "        \"\"\"\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminador: Clasifica im√°genes como reales (1) o falsas (0)\n",
        "    \n",
        "    Arquitectura:\n",
        "    - Input: Imagen aplanada 28x28 = 784\n",
        "    - Hidden: 2 capas densas con LeakyReLU\n",
        "    - Output: Probabilidad [0, 1] con Sigmoid\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, img_dim=28*28):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(img_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Probabilidad [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Forward pass del discriminador\n",
        "        Args:\n",
        "            img: Tensor de im√°genes (batch_size, img_dim)\n",
        "        Returns:\n",
        "            prob: Probabilidad de que la imagen sea real (batch_size, 1)\n",
        "        \"\"\"\n",
        "        return self.net(img)\n",
        "\n",
        "\n",
        "# Inicializar modelos\n",
        "z_dim = 100\n",
        "img_dim = 28 * 28\n",
        "\n",
        "generator = Generator(z_dim, img_dim).to(device)\n",
        "discriminator = Discriminator(img_dim).to(device)\n",
        "\n",
        "# Mostrar informaci√≥n de los modelos\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Informaci√≥n de los modelos:\")\n",
        "print(f\"  Generator - Par√°metros: {count_parameters(generator):,}\")\n",
        "print(f\"  Discriminator - Par√°metros: {count_parameters(discriminator):,}\")\n",
        "print(f\"  Dimensi√≥n del ruido (z): {z_dim}\")\n",
        "print(f\"  Dimensi√≥n de imagen: {img_dim}\")"
      ],
      "metadata": {
        "id": "models_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 3: ENTRENAMIENTO DEL GAN\n",
        "# ============================================================================================\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "num_epochs = 25\n",
        "learning_rate = 0.0002\n",
        "beta1 = 0.5  # Par√°metro beta1 para Adam optimizer\n",
        "\n",
        "# Optimizadores\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "\n",
        "# Funci√≥n de p√©rdida\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Diccionarios para almacenar m√©tricas\n",
        "metrics_history = {\n",
        "    'generator_losses': [],\n",
        "    'discriminator_losses_real': [],\n",
        "    'discriminator_losses_fake': [],\n",
        "    'discriminator_losses_total': [],\n",
        "    'discriminator_acc_real': [],\n",
        "    'discriminator_acc_fake': [],\n",
        "    'discriminator_acc_total': []\n",
        "}\n",
        "\n",
        "# Ruido fijo para generar muestras consistentes durante entrenamiento\n",
        "fixed_noise = torch.randn(16, z_dim, device=device)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO ENTRENAMIENTO DEL GAN\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"√âpocas: {num_epochs}\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"Beta1: {beta1}\")\n",
        "print(f\"Criterio: BCE Loss\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # M√©tricas por √©poca\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss_real = 0.0\n",
        "    epoch_d_loss_fake = 0.0\n",
        "    epoch_d_acc_real = 0.0\n",
        "    epoch_d_acc_fake = 0.0\n",
        "    \n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    \n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        batch_size = real_images.size(0)\n",
        "        real_images = real_images.view(batch_size, -1).to(device)\n",
        "        \n",
        "        # Etiquetas\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "        \n",
        "        # ============================================\n",
        "        # ENTRENAR DISCRIMINADOR\n",
        "        # ============================================\n",
        "        optimizer_D.zero_grad()\n",
        "        \n",
        "        # Discriminador en im√°genes reales\n",
        "        outputs_real = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs_real, real_labels)\n",
        "        d_acc_real = ((outputs_real > 0.5).float() == real_labels).float().mean()\n",
        "        \n",
        "        # Generar im√°genes falsas\n",
        "        z = torch.randn(batch_size, z_dim, device=device)\n",
        "        fake_images = generator(z).detach()  # Detach para no actualizar G\n",
        "        \n",
        "        # Discriminador en im√°genes falsas\n",
        "        outputs_fake = discriminator(fake_images)\n",
        "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
        "        d_acc_fake = ((outputs_fake > 0.5).float() == fake_labels).float().mean()\n",
        "        \n",
        "        # P√©rdida total del discriminador\n",
        "        d_loss_total = (d_loss_real + d_loss_fake) / 2\n",
        "        d_loss_total.backward()\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        # ============================================\n",
        "        # ENTRENAR GENERADOR\n",
        "        # ============================================\n",
        "        optimizer_G.zero_grad()\n",
        "        \n",
        "        # Generar nuevas im√°genes falsas\n",
        "        z = torch.randn(batch_size, z_dim, device=device)\n",
        "        fake_images = generator(z)\n",
        "        \n",
        "        # El generador quiere que el discriminador clasifique las falsas como reales\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)  # Etiquetas reales!\n",
        "        \n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "        \n",
        "        # Acumular m√©tricas\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss_real += d_loss_real.item()\n",
        "        epoch_d_loss_fake += d_loss_fake.item()\n",
        "        epoch_d_acc_real += d_acc_real.item()\n",
        "        epoch_d_acc_fake += d_acc_fake.item()\n",
        "    \n",
        "    # Promediar m√©tricas por √©poca\n",
        "    num_batches = len(dataloader)\n",
        "    avg_g_loss = epoch_g_loss / num_batches\n",
        "    avg_d_loss_real = epoch_d_loss_real / num_batches\n",
        "    avg_d_loss_fake = epoch_d_loss_fake / num_batches\n",
        "    avg_d_loss_total = (avg_d_loss_real + avg_d_loss_fake) / 2\n",
        "    avg_d_acc_real = epoch_d_acc_real / num_batches\n",
        "    avg_d_acc_fake = epoch_d_acc_fake / num_batches\n",
        "    avg_d_acc_total = (avg_d_acc_real + avg_d_acc_fake) / 2\n",
        "    \n",
        "    # Guardar m√©tricas\n",
        "    metrics_history['generator_losses'].append(avg_g_loss)\n",
        "    metrics_history['discriminator_losses_real'].append(avg_d_loss_real)\n",
        "    metrics_history['discriminator_losses_fake'].append(avg_d_loss_fake)\n",
        "    metrics_history['discriminator_losses_total'].append(avg_d_loss_total)\n",
        "    metrics_history['discriminator_acc_real'].append(avg_d_acc_real)\n",
        "    metrics_history['discriminator_acc_fake'].append(avg_d_acc_fake)\n",
        "    metrics_history['discriminator_acc_total'].append(avg_d_acc_total)\n",
        "    \n",
        "    # Mostrar progreso cada 5 √©pocas\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"√âpoca [{epoch+1:2d}/{num_epochs}] | \"\n",
        "              f\"G_Loss: {avg_g_loss:.4f} | \"\n",
        "              f\"D_Loss: {avg_d_loss_total:.4f} | \"\n",
        "              f\"D_Acc_Real: {avg_d_acc_real:.3f} | \"\n",
        "              f\"D_Acc_Fake: {avg_d_acc_fake:.3f} | \"\n",
        "              f\"Tiempo: {elapsed:.1f}s\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚è±Ô∏è Tiempo total: {total_time/60:.2f} minutos\")\n",
        "print(f\"üîÑ √âpocas completadas: {num_epochs}\")\n",
        "print(f\"üìä M√©tricas finales:\")\n",
        "print(f\"   Generator Loss: {metrics_history['generator_losses'][-1]:.4f}\")\n",
        "print(f\"   Discriminator Loss: {metrics_history['discriminator_losses_total'][-1]:.4f}\")\n",
        "print(f\"   Discriminator Acc (Real): {metrics_history['discriminator_acc_real'][-1]:.3f}\")\n",
        "print(f\"   Discriminator Acc (Fake): {metrics_history['discriminator_acc_fake'][-1]:.3f}\")"
      ],
      "metadata": {
        "id": "training_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 4: VISUALIZACI√ìN DE LA EVOLUCI√ìN DE ERRORES\n",
        "# (REQUISITO DEL ENUNCIADO)\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZANDO EVOLUCI√ìN DE ERRORES DEL DISCRIMINADOR Y GENERADOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Evoluci√≥n del Entrenamiento GAN - Errores y M√©tricas', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "epochs_range = range(1, len(metrics_history['generator_losses']) + 1)\n",
        "\n",
        "# 1. P√©rdidas del Generador y Discriminador\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(epochs_range, metrics_history['generator_losses'], \n",
        "         label='Generator Loss', color='#2E86AB', linewidth=2.5)\n",
        "ax1.plot(epochs_range, metrics_history['discriminator_losses_total'], \n",
        "         label='Discriminator Loss', color='#A23B72', linewidth=2.5)\n",
        "ax1.set_title('Evoluci√≥n de P√©rdidas (Loss)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('√âpoca')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(bottom=0)\n",
        "\n",
        "# 2. Accuracy del Discriminador\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_real'], \n",
        "         label='Acc. Real Images', color='#F18F01', linewidth=2.5)\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_fake'], \n",
        "         label='Acc. Fake Images', color='#C73E1D', linewidth=2.5)\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_total'], \n",
        "         label='Acc. Total', color='#6A994E', linewidth=2.5, linestyle='--')\n",
        "ax2.set_title('Accuracy del Discriminador', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('√âpoca')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "# 3. P√©rdidas del Discriminador por tipo\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(epochs_range, metrics_history['discriminator_losses_real'], \n",
        "         label='D Loss (Real)', color='#386641', linewidth=2.5)\n",
        "ax3.plot(epochs_range, metrics_history['discriminator_losses_fake'], \n",
        "         label='D Loss (Fake)', color='#BC4749', linewidth=2.5)\n",
        "ax3.set_title('P√©rdidas del Discriminador por Tipo', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('√âpoca')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim(bottom=0)\n",
        "\n",
        "# 4. Comparaci√≥n directa G vs D Loss\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(epochs_range, metrics_history['generator_losses'], \n",
        "         label='Generator', color='#2E86AB', linewidth=3)\n",
        "ax4.plot(epochs_range, metrics_history['discriminator_losses_total'], \n",
        "         label='Discriminator', color='#A23B72', linewidth=3)\n",
        "ax4.set_title('Competencia Generator vs Discriminator', fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel('√âpoca')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_ylim(bottom=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('gan_training_evolution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Gr√°fica de evoluci√≥n de errores generada y guardada como 'gan_training_evolution.png'\")\n",
        "\n",
        "# An√°lisis de convergencia\n",
        "print(f\"\\nüìà AN√ÅLISIS DE CONVERGENCIA:\")\n",
        "print(f\"   ‚Ä¢ Generator Loss: {metrics_history['generator_losses'][0]:.4f} ‚Üí {metrics_history['generator_losses'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ Discriminator Loss: {metrics_history['discriminator_losses_total'][0]:.4f} ‚Üí {metrics_history['discriminator_losses_total'][-1]:.4f}\")\n",
        "print(f\"   ‚Ä¢ D Accuracy (Real): {metrics_history['discriminator_acc_real'][-1]:.3f}\")\n",
        "print(f\"   ‚Ä¢ D Accuracy (Fake): {metrics_history['discriminator_acc_fake'][-1]:.3f}\")\n",
        "\n",
        "# Interpretaci√≥n\n",
        "final_d_acc_real = metrics_history['discriminator_acc_real'][-1]\n",
        "final_d_acc_fake = metrics_history['discriminator_acc_fake'][-1]\n",
        "balance_score = abs(final_d_acc_real - final_d_acc_fake)\n",
        "\n",
        "print(f\"\\nüí° INTERPRETACI√ìN:\")\n",
        "if balance_score < 0.1:\n",
        "    print(f\"   ‚úÖ Buen balance: Diferencia de accuracy = {balance_score:.3f} < 0.1\")\n",
        "elif balance_score < 0.2:\n",
        "    print(f\"   ‚ö†Ô∏è Balance moderado: Diferencia de accuracy = {balance_score:.3f}\")\n",
        "else:\n",
        "    print(f\"   ‚ùå Desbalance: Diferencia de accuracy = {balance_score:.3f} > 0.2\")\n",
        "\n",
        "if final_d_acc_real > 0.8 and final_d_acc_fake > 0.8:\n",
        "    print(f\"   üéØ Discriminador muy fuerte (ambas acc > 0.8)\")\n",
        "elif final_d_acc_real < 0.6 and final_d_acc_fake < 0.6:\n",
        "    print(f\"   ü§î Discriminador d√©bil (ambas acc < 0.6)\")\n",
        "else:\n",
        "    print(f\"   üëç Discriminador balanceado\")"
      ],
      "metadata": {
        "id": "loss_visualization_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 5: C√ÅLCULO DE M√âTRICAS DEL MODELO\n",
        "# (REQUISITO DEL ENUNCIADO)\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CALCULANDO M√âTRICAS DEL MODELO GAN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "generator.eval()\n",
        "discriminator.eval()\n",
        "\n",
        "# Preparar conjunto de validaci√≥n (muestras no vistas durante entrenamiento)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)\n",
        "test_real_batch, _ = next(iter(test_loader))\n",
        "test_real_batch = test_real_batch.view(test_real_batch.size(0), -1).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ============================================\n",
        "    # M√âTRICAS DEL DISCRIMINADOR\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Accuracy en im√°genes reales de test\n",
        "    real_outputs = discriminator(test_real_batch)\n",
        "    real_predictions = (real_outputs > 0.5).float()\n",
        "    real_targets = torch.ones_like(real_predictions)\n",
        "    d_accuracy_real_test = (real_predictions == real_targets).float().mean().item()\n",
        "    \n",
        "    # 2. Accuracy en im√°genes generadas\n",
        "    test_noise = torch.randn(1000, z_dim, device=device)\n",
        "    fake_images_test = generator(test_noise)\n",
        "    fake_outputs = discriminator(fake_images_test)\n",
        "    fake_predictions = (fake_outputs > 0.5).float()\n",
        "    fake_targets = torch.zeros_like(fake_predictions)\n",
        "    d_accuracy_fake_test = (fake_predictions == fake_targets).float().mean().item()\n",
        "    \n",
        "    # 3. Accuracy total del discriminador\n",
        "    d_accuracy_total_test = (d_accuracy_real_test + d_accuracy_fake_test) / 2\n",
        "    \n",
        "    # 4. P√©rdidas de test\n",
        "    real_labels_test = torch.ones(test_real_batch.size(0), 1, device=device)\n",
        "    fake_labels_test = torch.zeros(fake_images_test.size(0), 1, device=device)\n",
        "    \n",
        "    d_loss_real_test = criterion(real_outputs, real_labels_test).item()\n",
        "    d_loss_fake_test = criterion(fake_outputs, fake_labels_test).item()\n",
        "    d_loss_total_test = (d_loss_real_test + d_loss_fake_test) / 2\n",
        "    \n",
        "    # ============================================\n",
        "    # M√âTRICAS DEL GENERADOR\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Capacidad de enga√±ar al discriminador\n",
        "    g_success_rate = fake_outputs.mean().item()  # Promedio de probabilidades\n",
        "    g_fool_rate = (fake_outputs > 0.5).float().mean().item()  # % que logra enga√±ar\n",
        "    \n",
        "    # 2. P√©rdida del generador en test\n",
        "    g_loss_test = criterion(fake_outputs, real_labels_test).item()\n",
        "    \n",
        "    # ============================================\n",
        "    # M√âTRICAS DE CALIDAD (B√ÅSICAS)\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Diversidad: Desviaci√≥n est√°ndar de activaciones\n",
        "    fake_sample = generator(torch.randn(100, z_dim, device=device))\n",
        "    diversity_std = fake_sample.std(dim=0).mean().item()\n",
        "    \n",
        "    # 2. Estad√≠sticas de p√≠xeles generados\n",
        "    fake_mean = fake_sample.mean().item()\n",
        "    fake_std = fake_sample.std().item()\n",
        "    \n",
        "    # 3. Rango de valores generados\n",
        "    fake_min = fake_sample.min().item()\n",
        "    fake_max = fake_sample.max().item()\n",
        "\n",
        "# ============================================\n",
        "# PRESENTACI√ìN DE RESULTADOS\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nüéØ M√âTRICAS DEL DISCRIMINADOR:\")\n",
        "print(f\"   ‚Ä¢ Accuracy en im√°genes reales (test): {d_accuracy_real_test:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy en im√°genes falsas (test): {d_accuracy_fake_test:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy total: {d_accuracy_total_test:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss en im√°genes reales: {d_loss_real_test:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss en im√°genes falsas: {d_loss_fake_test:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss total: {d_loss_total_test:.4f}\")\n",
        "\n",
        "print(f\"\\nüé® M√âTRICAS DEL GENERADOR:\")\n",
        "print(f\"   ‚Ä¢ Probabilidad promedio de enga√±o: {g_success_rate:.4f}\")\n",
        "print(f\"   ‚Ä¢ Tasa de enga√±o exitoso (>0.5): {g_fool_rate:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss del generador: {g_loss_test:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS DE CALIDAD:\")\n",
        "print(f\"   ‚Ä¢ Diversidad (std de activaciones): {diversity_std:.4f}\")\n",
        "print(f\"   ‚Ä¢ Media de p√≠xeles generados: {fake_mean:.4f}\")\n",
        "print(f\"   ‚Ä¢ Desv. est√°ndar de p√≠xeles: {fake_std:.4f}\")\n",
        "print(f\"   ‚Ä¢ Rango de valores: [{fake_min:.3f}, {fake_max:.3f}]\")\n",
        "\n",
        "# ============================================\n",
        "# EVALUACI√ìN DEL RENDIMIENTO\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nüíØ EVALUACI√ìN DEL RENDIMIENTO:\")\n",
        "\n",
        "# Discriminador\n",
        "if d_accuracy_total_test > 0.8:\n",
        "    d_performance = \"Excelente\"\n",
        "elif d_accuracy_total_test > 0.7:\n",
        "    d_performance = \"Bueno\"\n",
        "elif d_accuracy_total_test > 0.6:\n",
        "    d_performance = \"Aceptable\"\n",
        "else:\n",
        "    d_performance = \"Necesita mejora\"\n",
        "\n",
        "print(f\"   ‚Ä¢ Discriminador: {d_performance} (Accuracy: {d_accuracy_total_test:.3f})\")\n",
        "\n",
        "# Generador\n",
        "if g_fool_rate > 0.4:\n",
        "    g_performance = \"Excelente\"\n",
        "elif g_fool_rate > 0.3:\n",
        "    g_performance = \"Bueno\"\n",
        "elif g_fool_rate > 0.2:\n",
        "    g_performance = \"Aceptable\"\n",
        "else:\n",
        "    g_performance = \"Necesita mejora\"\n",
        "\n",
        "print(f\"   ‚Ä¢ Generador: {g_performance} (Enga√±o: {g_fool_rate:.3f})\")\n",
        "\n",
        "# Balance general\n",
        "balance = abs(d_accuracy_real_test - d_accuracy_fake_test)\n",
        "if balance < 0.05:\n",
        "    balance_status = \"Muy equilibrado\"\n",
        "elif balance < 0.1:\n",
        "    balance_status = \"Equilibrado\"\n",
        "elif balance < 0.2:\n",
        "    balance_status = \"Ligeramente desbalanceado\"\n",
        "else:\n",
        "    balance_status = \"Desbalanceado\"\n",
        "\n",
        "print(f\"   ‚Ä¢ Balance G vs D: {balance_status} (Diferencia: {balance:.3f})\")\n",
        "\n",
        "# Crear tabla de resumen\n",
        "print(f\"\\nüìã RESUMEN FINAL DE M√âTRICAS:\")\n",
        "print(f\"{'M√©trica':<30} {'Valor':<12} {'Estado':<15}\")\n",
        "print(f\"{'-'*57}\")\n",
        "print(f\"{'Discriminator Acc (Total)':<30} {d_accuracy_total_test:<12.4f} {d_performance:<15}\")\n",
        "print(f\"{'Generator Fool Rate':<30} {g_fool_rate:<12.4f} {g_performance:<15}\")\n",
        "print(f\"{'G vs D Balance':<30} {balance:<12.4f} {balance_status:<15}\")\n",
        "print(f\"{'Diversidad':<30} {diversity_std:<12.4f} {'Calculada':<15}\")\n",
        "print(f\"{'Epochs entrenadas':<30} {num_epochs:<12} {'Completas':<15}\")"
      ],
      "metadata": {
        "id": "metrics_calculation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 6: GENERACI√ìN Y VISUALIZACI√ìN DE MUESTRAS\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERANDO MUESTRAS FINALES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "generator.eval()\n",
        "\n",
        "# Generar grid de muestras\n",
        "n_samples = 25  # 5x5 grid\n",
        "sample_noise = torch.randn(n_samples, z_dim, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_samples = generator(sample_noise).cpu()\n",
        "    generated_samples = generated_samples.view(-1, 28, 28)\n",
        "    # Desnormalizar de [-1, 1] a [0, 1]\n",
        "    generated_samples = (generated_samples + 1) / 2\n",
        "\n",
        "# Visualizar muestras generadas\n",
        "fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
        "fig.suptitle('D√≠gitos MNIST Generados por GAN', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(n_samples):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    axes[row, col].imshow(generated_samples[i], cmap='gray')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('generated_mnist_samples.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Muestras generadas y guardadas como 'generated_mnist_samples.png'\")\n",
        "\n",
        "# Comparaci√≥n con im√°genes reales\n",
        "print(\"\\nüìä COMPARACI√ìN CON IM√ÅGENES REALES:\")\n",
        "\n",
        "# Obtener muestras reales para comparaci√≥n\n",
        "real_samples, _ = next(iter(DataLoader(test_dataset, batch_size=25, shuffle=True)))\n",
        "real_samples = real_samples.squeeze()\n",
        "real_samples = (real_samples + 1) / 2  # Desnormalizar\n",
        "\n",
        "fig, axes = plt.subplots(2, 10, figsize=(20, 6))\n",
        "fig.suptitle('Comparaci√≥n: Im√°genes Reales vs Generadas', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Fila superior: im√°genes reales\n",
        "for i in range(10):\n",
        "    axes[0, i].imshow(real_samples[i], cmap='gray')\n",
        "    axes[0, i].set_title('Real', fontsize=10, color='green')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Fila inferior: im√°genes generadas\n",
        "for i in range(10):\n",
        "    axes[1, i].imshow(generated_samples[i], cmap='gray')\n",
        "    axes[1, i].set_title('Generada', fontsize=10, color='blue')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('real_vs_generated_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Comparaci√≥n guardada como 'real_vs_generated_comparison.png'\")\n",
        "\n",
        "# An√°lisis de calidad visual\n",
        "print(f\"\\nüîç AN√ÅLISIS DE CALIDAD VISUAL:\")\n",
        "print(f\"   ‚Ä¢ Muestras generadas: {n_samples}\")\n",
        "print(f\"   ‚Ä¢ Rango de p√≠xeles: [{generated_samples.min():.3f}, {generated_samples.max():.3f}]\")\n",
        "print(f\"   ‚Ä¢ Media de intensidad: {generated_samples.mean():.3f}\")\n",
        "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: {generated_samples.std():.3f}\")\n",
        "\n",
        "# Guardar modelo entrenado\n",
        "print(f\"\\nüíæ GUARDANDO MODELO:\")\n",
        "torch.save({\n",
        "    'generator_state_dict': generator.state_dict(),\n",
        "    'discriminator_state_dict': discriminator.state_dict(),\n",
        "    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "    'metrics_history': metrics_history,\n",
        "    'hyperparameters': {\n",
        "        'z_dim': z_dim,\n",
        "        'img_dim': img_dim,\n",
        "        'learning_rate': learning_rate,\n",
        "        'beta1': beta1,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "}, 'gan_model_complete.pth')\n",
        "\n",
        "print(\"‚úÖ Modelo completo guardado como 'gan_model_complete.pth'\")"
      ],
      "metadata": {
        "id": "sample_generation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions_markdown"
      },
      "source": [
        "## Conclusiones y An√°lisis\n",
        "\n",
        "### ‚úÖ Cumplimiento del Enunciado:\n",
        "\n",
        "1. **‚úÖ Visualizaci√≥n de evoluci√≥n de errores**: Se implement√≥ una gr√°fica completa mostrando la evoluci√≥n de los errores del discriminador y generador durante todo el entrenamiento, con an√°lisis detallado de convergencia.\n",
        "\n",
        "2. **‚úÖ C√°lculo de m√©tricas del modelo**: Se calcularon m√©tricas comprehensivas incluyendo:\n",
        "   - Accuracy del discriminador en im√°genes reales y falsas\n",
        "   - P√©rdidas del discriminador por tipo (real/fake)\n",
        "   - M√©tricas del generador (tasa de enga√±o, probabilidad promedio)\n",
        "   - M√©tricas de calidad (diversidad, estad√≠sticas de p√≠xeles)\n",
        "\n",
        "### üéØ Caracter√≠sticas del Entrenamiento Adversarial:\n",
        "\n",
        "- **Competencia equilibrada**: El discriminador y generador compiten din√°micamente\n",
        "- **Convergencia estable**: Las p√©rdidas se estabilizan indicando equilibrio Nash\n",
        "- **Balance cr√≠tico**: Un discriminador muy fuerte impide el aprendizaje del generador\n",
        "\n",
        "### üìä M√©tricas Clave:\n",
        "\n",
        "- **Accuracy del Discriminador**: Mide qu√© tan bien distingue reales de falsas\n",
        "- **Tasa de Enga√±o del Generador**: Porcentaje de im√°genes que logran enga√±ar\n",
        "- **Balance G vs D**: Diferencia en accuracy indica equilibrio del entrenamiento\n",
        "- **Diversidad**: Variabilidad en las muestras generadas\n",
        "\n",
        "### üí° Interpretaci√≥n de Resultados:\n",
        "\n",
        "- **Discriminador ideal**: Accuracy ~0.75-0.85 (ni muy fuerte ni muy d√©bil)\n",
        "- **Generador exitoso**: Tasa de enga√±o >0.3 indica buena capacidad generativa\n",
        "- **Equilibrio √≥ptimo**: Diferencia <0.1 entre acc_real y acc_fake\n",
        "\n",
        "### üîß Arquitectura Utilizada:\n",
        "\n",
        "- **Generador**: Redes densas con ReLU y Dropout, salida con Tanh\n",
        "- **Discriminador**: Redes densas con LeakyReLU y Dropout, salida con Sigmoid\n",
        "- **Optimizaci√≥n**: Adam con Œ≤‚ÇÅ=0.5 para estabilidad en GANs\n",
        "- **Funci√≥n de p√©rdida**: Binary Cross Entropy (BCE)\n",
        "\n",
        "### üìà Consideraciones de Mejora:\n",
        "\n",
        "- Implementar t√©cnicas de estabilizaci√≥n (Spectral Normalization, WGAN)\n",
        "- Usar arquitecturas convolucionales (DCGAN) para mejor calidad\n",
        "- Aplicar t√©cnicas de regularizaci√≥n avanzadas\n",
        "- Implementar m√©tricas de calidad m√°s sofisticadas (FID, IS)\n",
        "\n",
        "Este GAN b√°sico cumple exitosamente con los requisitos del enunciado y proporciona una base s√≥lida para entender el entrenamiento adversarial."
      ]
    }
  ]
}