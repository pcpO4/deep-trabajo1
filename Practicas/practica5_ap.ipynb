{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Práctica 5: Generative Adversarial Network (GAN) para MNIST\n",
        "\n",
        "En esta práctica implementamos un GAN simple para generar dígitos MNIST. Seguimos el enunciado que requiere:\n",
        "\n",
        "## Objetivos del enunciado:\n",
        "1. **Visualizar la evolución de errores**: Gráfica con la evolución de los errores del discriminador y generador durante el entrenamiento\n",
        "2. **Calcular métricas del modelo**: Métricas de rendimiento del discriminador y generador\n",
        "\n",
        "## Arquitectura GAN:\n",
        "- **Generador**: Transforma ruido aleatorio en imágenes 28x28\n",
        "- **Discriminador**: Clasifica imágenes como reales o generadas\n",
        "- **Entrenamiento adversarial**: Ambos modelos compiten para mejorar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 1: CONFIGURACIÓN E IMPORTACIÓN DE LIBRERÍAS\n",
        "# ============================================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de reproducibilidad\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuración de dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
        "\n",
        "# Configuración del dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalizar a [-1, 1]\n",
        "])\n",
        "\n",
        "# Cargar dataset MNIST\n",
        "print(\"\\nCargando dataset MNIST...\")\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(mnist_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"Dataset cargado: {len(mnist_dataset)} muestras\")\n",
        "print(f\"Batches por época: {len(dataloader)}\")"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 2: DEFINICIÓN DE ARQUITECTURAS\n",
        "# ============================================================================================\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generador: Convierte ruido aleatorio (z) en imágenes MNIST 28x28\n",
        "    \n",
        "    Arquitectura:\n",
        "    - Input: Vector de ruido de dimensión z_dim (100)\n",
        "    - Hidden: 2 capas densas con ReLU\n",
        "    - Output: Imagen 28x28 con activación Tanh [-1, 1]\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, z_dim=100, img_dim=28*28):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(512, img_dim),\n",
        "            nn.Tanh()  # Output en [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Forward pass del generador\n",
        "        Args:\n",
        "            z: Tensor de ruido (batch_size, z_dim)\n",
        "        Returns:\n",
        "            img: Tensor de imágenes generadas (batch_size, img_dim)\n",
        "        \"\"\"\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminador: Clasifica imágenes como reales (1) o falsas (0)\n",
        "    \n",
        "    Arquitectura:\n",
        "    - Input: Imagen aplanada 28x28 = 784\n",
        "    - Hidden: 2 capas densas con LeakyReLU\n",
        "    - Output: Probabilidad [0, 1] con Sigmoid\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, img_dim=28*28):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(img_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Probabilidad [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Forward pass del discriminador\n",
        "        Args:\n",
        "            img: Tensor de imágenes (batch_size, img_dim)\n",
        "        Returns:\n",
        "            prob: Probabilidad de que la imagen sea real (batch_size, 1)\n",
        "        \"\"\"\n",
        "        return self.net(img)\n",
        "\n",
        "\n",
        "# Inicializar modelos\n",
        "z_dim = 100\n",
        "img_dim = 28 * 28\n",
        "\n",
        "generator = Generator(z_dim, img_dim).to(device)\n",
        "discriminator = Discriminator(img_dim).to(device)\n",
        "\n",
        "# Mostrar información de los modelos\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n📊 Información de los modelos:\")\n",
        "print(f\"  Generator - Parámetros: {count_parameters(generator):,}\")\n",
        "print(f\"  Discriminator - Parámetros: {count_parameters(discriminator):,}\")\n",
        "print(f\"  Dimensión del ruido (z): {z_dim}\")\n",
        "print(f\"  Dimensión de imagen: {img_dim}\")"
      ],
      "metadata": {
        "id": "models_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 3: ENTRENAMIENTO DEL GAN\n",
        "# ============================================================================================\n",
        "\n",
        "# Configuración de entrenamiento\n",
        "num_epochs = 25\n",
        "learning_rate = 0.0002\n",
        "beta1 = 0.5  # Parámetro beta1 para Adam optimizer\n",
        "\n",
        "# Optimizadores\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "\n",
        "# Función de pérdida\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Diccionarios para almacenar métricas\n",
        "metrics_history = {\n",
        "    'generator_losses': [],\n",
        "    'discriminator_losses_real': [],\n",
        "    'discriminator_losses_fake': [],\n",
        "    'discriminator_losses_total': [],\n",
        "    'discriminator_acc_real': [],\n",
        "    'discriminator_acc_fake': [],\n",
        "    'discriminator_acc_total': []\n",
        "}\n",
        "\n",
        "# Ruido fijo para generar muestras consistentes durante entrenamiento\n",
        "fixed_noise = torch.randn(16, z_dim, device=device)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO ENTRENAMIENTO DEL GAN\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Épocas: {num_epochs}\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"Beta1: {beta1}\")\n",
        "print(f\"Criterio: BCE Loss\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Métricas por época\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss_real = 0.0\n",
        "    epoch_d_loss_fake = 0.0\n",
        "    epoch_d_acc_real = 0.0\n",
        "    epoch_d_acc_fake = 0.0\n",
        "    \n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    \n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        batch_size = real_images.size(0)\n",
        "        real_images = real_images.view(batch_size, -1).to(device)\n",
        "        \n",
        "        # Etiquetas\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "        \n",
        "        # ============================================\n",
        "        # ENTRENAR DISCRIMINADOR\n",
        "        # ============================================\n",
        "        optimizer_D.zero_grad()\n",
        "        \n",
        "        # Discriminador en imágenes reales\n",
        "        outputs_real = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs_real, real_labels)\n",
        "        d_acc_real = ((outputs_real > 0.5).float() == real_labels).float().mean()\n",
        "        \n",
        "        # Generar imágenes falsas\n",
        "        z = torch.randn(batch_size, z_dim, device=device)\n",
        "        fake_images = generator(z).detach()  # Detach para no actualizar G\n",
        "        \n",
        "        # Discriminador en imágenes falsas\n",
        "        outputs_fake = discriminator(fake_images)\n",
        "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
        "        d_acc_fake = ((outputs_fake > 0.5).float() == fake_labels).float().mean()\n",
        "        \n",
        "        # Pérdida total del discriminador\n",
        "        d_loss_total = (d_loss_real + d_loss_fake) / 2\n",
        "        d_loss_total.backward()\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        # ============================================\n",
        "        # ENTRENAR GENERADOR\n",
        "        # ============================================\n",
        "        optimizer_G.zero_grad()\n",
        "        \n",
        "        # Generar nuevas imágenes falsas\n",
        "        z = torch.randn(batch_size, z_dim, device=device)\n",
        "        fake_images = generator(z)\n",
        "        \n",
        "        # El generador quiere que el discriminador clasifique las falsas como reales\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)  # Etiquetas reales!\n",
        "        \n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "        \n",
        "        # Acumular métricas\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss_real += d_loss_real.item()\n",
        "        epoch_d_loss_fake += d_loss_fake.item()\n",
        "        epoch_d_acc_real += d_acc_real.item()\n",
        "        epoch_d_acc_fake += d_acc_fake.item()\n",
        "    \n",
        "    # Promediar métricas por época\n",
        "    num_batches = len(dataloader)\n",
        "    avg_g_loss = epoch_g_loss / num_batches\n",
        "    avg_d_loss_real = epoch_d_loss_real / num_batches\n",
        "    avg_d_loss_fake = epoch_d_loss_fake / num_batches\n",
        "    avg_d_loss_total = (avg_d_loss_real + avg_d_loss_fake) / 2\n",
        "    avg_d_acc_real = epoch_d_acc_real / num_batches\n",
        "    avg_d_acc_fake = epoch_d_acc_fake / num_batches\n",
        "    avg_d_acc_total = (avg_d_acc_real + avg_d_acc_fake) / 2\n",
        "    \n",
        "    # Guardar métricas\n",
        "    metrics_history['generator_losses'].append(avg_g_loss)\n",
        "    metrics_history['discriminator_losses_real'].append(avg_d_loss_real)\n",
        "    metrics_history['discriminator_losses_fake'].append(avg_d_loss_fake)\n",
        "    metrics_history['discriminator_losses_total'].append(avg_d_loss_total)\n",
        "    metrics_history['discriminator_acc_real'].append(avg_d_acc_real)\n",
        "    metrics_history['discriminator_acc_fake'].append(avg_d_acc_fake)\n",
        "    metrics_history['discriminator_acc_total'].append(avg_d_acc_total)\n",
        "    \n",
        "    # Mostrar progreso cada 5 épocas\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Época [{epoch+1:2d}/{num_epochs}] | \"\n",
        "              f\"G_Loss: {avg_g_loss:.4f} | \"\n",
        "              f\"D_Loss: {avg_d_loss_total:.4f} | \"\n",
        "              f\"D_Acc_Real: {avg_d_acc_real:.3f} | \"\n",
        "              f\"D_Acc_Fake: {avg_d_acc_fake:.3f} | \"\n",
        "              f\"Tiempo: {elapsed:.1f}s\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"⏱️ Tiempo total: {total_time/60:.2f} minutos\")\n",
        "print(f\"🔄 Épocas completadas: {num_epochs}\")\n",
        "print(f\"📊 Métricas finales:\")\n",
        "print(f\"   Generator Loss: {metrics_history['generator_losses'][-1]:.4f}\")\n",
        "print(f\"   Discriminator Loss: {metrics_history['discriminator_losses_total'][-1]:.4f}\")\n",
        "print(f\"   Discriminator Acc (Real): {metrics_history['discriminator_acc_real'][-1]:.3f}\")\n",
        "print(f\"   Discriminator Acc (Fake): {metrics_history['discriminator_acc_fake'][-1]:.3f}\")"
      ],
      "metadata": {
        "id": "training_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 4: VISUALIZACIÓN DE LA EVOLUCIÓN DE ERRORES\n",
        "# (REQUISITO DEL ENUNCIADO)\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZANDO EVOLUCIÓN DE ERRORES DEL DISCRIMINADOR Y GENERADOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Evolución del Entrenamiento GAN - Errores y Métricas', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "epochs_range = range(1, len(metrics_history['generator_losses']) + 1)\n",
        "\n",
        "# 1. Pérdidas del Generador y Discriminador\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(epochs_range, metrics_history['generator_losses'], \n",
        "         label='Generator Loss', color='#2E86AB', linewidth=2.5)\n",
        "ax1.plot(epochs_range, metrics_history['discriminator_losses_total'], \n",
        "         label='Discriminator Loss', color='#A23B72', linewidth=2.5)\n",
        "ax1.set_title('Evolución de Pérdidas (Loss)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Época')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(bottom=0)\n",
        "\n",
        "# 2. Accuracy del Discriminador\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_real'], \n",
        "         label='Acc. Real Images', color='#F18F01', linewidth=2.5)\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_fake'], \n",
        "         label='Acc. Fake Images', color='#C73E1D', linewidth=2.5)\n",
        "ax2.plot(epochs_range, metrics_history['discriminator_acc_total'], \n",
        "         label='Acc. Total', color='#6A994E', linewidth=2.5, linestyle='--')\n",
        "ax2.set_title('Accuracy del Discriminador', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Época')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "# 3. Pérdidas del Discriminador por tipo\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(epochs_range, metrics_history['discriminator_losses_real'], \n",
        "         label='D Loss (Real)', color='#386641', linewidth=2.5)\n",
        "ax3.plot(epochs_range, metrics_history['discriminator_losses_fake'], \n",
        "         label='D Loss (Fake)', color='#BC4749', linewidth=2.5)\n",
        "ax3.set_title('Pérdidas del Discriminador por Tipo', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Época')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim(bottom=0)\n",
        "\n",
        "# 4. Comparación directa G vs D Loss\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(epochs_range, metrics_history['generator_losses'], \n",
        "         label='Generator', color='#2E86AB', linewidth=3)\n",
        "ax4.plot(epochs_range, metrics_history['discriminator_losses_total'], \n",
        "         label='Discriminator', color='#A23B72', linewidth=3)\n",
        "ax4.set_title('Competencia Generator vs Discriminator', fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel('Época')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_ylim(bottom=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('gan_training_evolution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Gráfica de evolución de errores generada y guardada como 'gan_training_evolution.png'\")\n",
        "\n",
        "# Análisis de convergencia\n",
        "print(f\"\\n📈 ANÁLISIS DE CONVERGENCIA:\")\n",
        "print(f\"   • Generator Loss: {metrics_history['generator_losses'][0]:.4f} → {metrics_history['generator_losses'][-1]:.4f}\")\n",
        "print(f\"   • Discriminator Loss: {metrics_history['discriminator_losses_total'][0]:.4f} → {metrics_history['discriminator_losses_total'][-1]:.4f}\")\n",
        "print(f\"   • D Accuracy (Real): {metrics_history['discriminator_acc_real'][-1]:.3f}\")\n",
        "print(f\"   • D Accuracy (Fake): {metrics_history['discriminator_acc_fake'][-1]:.3f}\")\n",
        "\n",
        "# Interpretación\n",
        "final_d_acc_real = metrics_history['discriminator_acc_real'][-1]\n",
        "final_d_acc_fake = metrics_history['discriminator_acc_fake'][-1]\n",
        "balance_score = abs(final_d_acc_real - final_d_acc_fake)\n",
        "\n",
        "print(f\"\\n💡 INTERPRETACIÓN:\")\n",
        "if balance_score < 0.1:\n",
        "    print(f\"   ✅ Buen balance: Diferencia de accuracy = {balance_score:.3f} < 0.1\")\n",
        "elif balance_score < 0.2:\n",
        "    print(f\"   ⚠️ Balance moderado: Diferencia de accuracy = {balance_score:.3f}\")\n",
        "else:\n",
        "    print(f\"   ❌ Desbalance: Diferencia de accuracy = {balance_score:.3f} > 0.2\")\n",
        "\n",
        "if final_d_acc_real > 0.8 and final_d_acc_fake > 0.8:\n",
        "    print(f\"   🎯 Discriminador muy fuerte (ambas acc > 0.8)\")\n",
        "elif final_d_acc_real < 0.6 and final_d_acc_fake < 0.6:\n",
        "    print(f\"   🤔 Discriminador débil (ambas acc < 0.6)\")\n",
        "else:\n",
        "    print(f\"   👍 Discriminador balanceado\")"
      ],
      "metadata": {
        "id": "loss_visualization_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 5: CÁLCULO DE MÉTRICAS DEL MODELO\n",
        "# (REQUISITO DEL ENUNCIADO)\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CALCULANDO MÉTRICAS DEL MODELO GAN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "generator.eval()\n",
        "discriminator.eval()\n",
        "\n",
        "# Preparar conjunto de validación (muestras no vistas durante entrenamiento)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)\n",
        "test_real_batch, _ = next(iter(test_loader))\n",
        "test_real_batch = test_real_batch.view(test_real_batch.size(0), -1).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ============================================\n",
        "    # MÉTRICAS DEL DISCRIMINADOR\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Accuracy en imágenes reales de test\n",
        "    real_outputs = discriminator(test_real_batch)\n",
        "    real_predictions = (real_outputs > 0.5).float()\n",
        "    real_targets = torch.ones_like(real_predictions)\n",
        "    d_accuracy_real_test = (real_predictions == real_targets).float().mean().item()\n",
        "    \n",
        "    # 2. Accuracy en imágenes generadas\n",
        "    test_noise = torch.randn(1000, z_dim, device=device)\n",
        "    fake_images_test = generator(test_noise)\n",
        "    fake_outputs = discriminator(fake_images_test)\n",
        "    fake_predictions = (fake_outputs > 0.5).float()\n",
        "    fake_targets = torch.zeros_like(fake_predictions)\n",
        "    d_accuracy_fake_test = (fake_predictions == fake_targets).float().mean().item()\n",
        "    \n",
        "    # 3. Accuracy total del discriminador\n",
        "    d_accuracy_total_test = (d_accuracy_real_test + d_accuracy_fake_test) / 2\n",
        "    \n",
        "    # 4. Pérdidas de test\n",
        "    real_labels_test = torch.ones(test_real_batch.size(0), 1, device=device)\n",
        "    fake_labels_test = torch.zeros(fake_images_test.size(0), 1, device=device)\n",
        "    \n",
        "    d_loss_real_test = criterion(real_outputs, real_labels_test).item()\n",
        "    d_loss_fake_test = criterion(fake_outputs, fake_labels_test).item()\n",
        "    d_loss_total_test = (d_loss_real_test + d_loss_fake_test) / 2\n",
        "    \n",
        "    # ============================================\n",
        "    # MÉTRICAS DEL GENERADOR\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Capacidad de engañar al discriminador\n",
        "    g_success_rate = fake_outputs.mean().item()  # Promedio de probabilidades\n",
        "    g_fool_rate = (fake_outputs > 0.5).float().mean().item()  # % que logra engañar\n",
        "    \n",
        "    # 2. Pérdida del generador en test\n",
        "    g_loss_test = criterion(fake_outputs, real_labels_test).item()\n",
        "    \n",
        "    # ============================================\n",
        "    # MÉTRICAS DE CALIDAD (BÁSICAS)\n",
        "    # ============================================\n",
        "    \n",
        "    # 1. Diversidad: Desviación estándar de activaciones\n",
        "    fake_sample = generator(torch.randn(100, z_dim, device=device))\n",
        "    diversity_std = fake_sample.std(dim=0).mean().item()\n",
        "    \n",
        "    # 2. Estadísticas de píxeles generados\n",
        "    fake_mean = fake_sample.mean().item()\n",
        "    fake_std = fake_sample.std().item()\n",
        "    \n",
        "    # 3. Rango de valores generados\n",
        "    fake_min = fake_sample.min().item()\n",
        "    fake_max = fake_sample.max().item()\n",
        "\n",
        "# ============================================\n",
        "# PRESENTACIÓN DE RESULTADOS\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n🎯 MÉTRICAS DEL DISCRIMINADOR:\")\n",
        "print(f\"   • Accuracy en imágenes reales (test): {d_accuracy_real_test:.4f}\")\n",
        "print(f\"   • Accuracy en imágenes falsas (test): {d_accuracy_fake_test:.4f}\")\n",
        "print(f\"   • Accuracy total: {d_accuracy_total_test:.4f}\")\n",
        "print(f\"   • Loss en imágenes reales: {d_loss_real_test:.4f}\")\n",
        "print(f\"   • Loss en imágenes falsas: {d_loss_fake_test:.4f}\")\n",
        "print(f\"   • Loss total: {d_loss_total_test:.4f}\")\n",
        "\n",
        "print(f\"\\n🎨 MÉTRICAS DEL GENERADOR:\")\n",
        "print(f\"   • Probabilidad promedio de engaño: {g_success_rate:.4f}\")\n",
        "print(f\"   • Tasa de engaño exitoso (>0.5): {g_fool_rate:.4f}\")\n",
        "print(f\"   • Loss del generador: {g_loss_test:.4f}\")\n",
        "\n",
        "print(f\"\\n📊 MÉTRICAS DE CALIDAD:\")\n",
        "print(f\"   • Diversidad (std de activaciones): {diversity_std:.4f}\")\n",
        "print(f\"   • Media de píxeles generados: {fake_mean:.4f}\")\n",
        "print(f\"   • Desv. estándar de píxeles: {fake_std:.4f}\")\n",
        "print(f\"   • Rango de valores: [{fake_min:.3f}, {fake_max:.3f}]\")\n",
        "\n",
        "# ============================================\n",
        "# EVALUACIÓN DEL RENDIMIENTO\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n💯 EVALUACIÓN DEL RENDIMIENTO:\")\n",
        "\n",
        "# Discriminador\n",
        "if d_accuracy_total_test > 0.8:\n",
        "    d_performance = \"Excelente\"\n",
        "elif d_accuracy_total_test > 0.7:\n",
        "    d_performance = \"Bueno\"\n",
        "elif d_accuracy_total_test > 0.6:\n",
        "    d_performance = \"Aceptable\"\n",
        "else:\n",
        "    d_performance = \"Necesita mejora\"\n",
        "\n",
        "print(f\"   • Discriminador: {d_performance} (Accuracy: {d_accuracy_total_test:.3f})\")\n",
        "\n",
        "# Generador\n",
        "if g_fool_rate > 0.4:\n",
        "    g_performance = \"Excelente\"\n",
        "elif g_fool_rate > 0.3:\n",
        "    g_performance = \"Bueno\"\n",
        "elif g_fool_rate > 0.2:\n",
        "    g_performance = \"Aceptable\"\n",
        "else:\n",
        "    g_performance = \"Necesita mejora\"\n",
        "\n",
        "print(f\"   • Generador: {g_performance} (Engaño: {g_fool_rate:.3f})\")\n",
        "\n",
        "# Balance general\n",
        "balance = abs(d_accuracy_real_test - d_accuracy_fake_test)\n",
        "if balance < 0.05:\n",
        "    balance_status = \"Muy equilibrado\"\n",
        "elif balance < 0.1:\n",
        "    balance_status = \"Equilibrado\"\n",
        "elif balance < 0.2:\n",
        "    balance_status = \"Ligeramente desbalanceado\"\n",
        "else:\n",
        "    balance_status = \"Desbalanceado\"\n",
        "\n",
        "print(f\"   • Balance G vs D: {balance_status} (Diferencia: {balance:.3f})\")\n",
        "\n",
        "# Crear tabla de resumen\n",
        "print(f\"\\n📋 RESUMEN FINAL DE MÉTRICAS:\")\n",
        "print(f\"{'Métrica':<30} {'Valor':<12} {'Estado':<15}\")\n",
        "print(f\"{'-'*57}\")\n",
        "print(f\"{'Discriminator Acc (Total)':<30} {d_accuracy_total_test:<12.4f} {d_performance:<15}\")\n",
        "print(f\"{'Generator Fool Rate':<30} {g_fool_rate:<12.4f} {g_performance:<15}\")\n",
        "print(f\"{'G vs D Balance':<30} {balance:<12.4f} {balance_status:<15}\")\n",
        "print(f\"{'Diversidad':<30} {diversity_std:<12.4f} {'Calculada':<15}\")\n",
        "print(f\"{'Epochs entrenadas':<30} {num_epochs:<12} {'Completas':<15}\")"
      ],
      "metadata": {
        "id": "metrics_calculation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCIÓN 6: GENERACIÓN Y VISUALIZACIÓN DE MUESTRAS\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERANDO MUESTRAS FINALES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "generator.eval()\n",
        "\n",
        "# Generar grid de muestras\n",
        "n_samples = 25  # 5x5 grid\n",
        "sample_noise = torch.randn(n_samples, z_dim, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_samples = generator(sample_noise).cpu()\n",
        "    generated_samples = generated_samples.view(-1, 28, 28)\n",
        "    # Desnormalizar de [-1, 1] a [0, 1]\n",
        "    generated_samples = (generated_samples + 1) / 2\n",
        "\n",
        "# Visualizar muestras generadas\n",
        "fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
        "fig.suptitle('Dígitos MNIST Generados por GAN', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(n_samples):\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    axes[row, col].imshow(generated_samples[i], cmap='gray')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('generated_mnist_samples.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Muestras generadas y guardadas como 'generated_mnist_samples.png'\")\n",
        "\n",
        "# Comparación con imágenes reales\n",
        "print(\"\\n📊 COMPARACIÓN CON IMÁGENES REALES:\")\n",
        "\n",
        "# Obtener muestras reales para comparación\n",
        "real_samples, _ = next(iter(DataLoader(test_dataset, batch_size=25, shuffle=True)))\n",
        "real_samples = real_samples.squeeze()\n",
        "real_samples = (real_samples + 1) / 2  # Desnormalizar\n",
        "\n",
        "fig, axes = plt.subplots(2, 10, figsize=(20, 6))\n",
        "fig.suptitle('Comparación: Imágenes Reales vs Generadas', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Fila superior: imágenes reales\n",
        "for i in range(10):\n",
        "    axes[0, i].imshow(real_samples[i], cmap='gray')\n",
        "    axes[0, i].set_title('Real', fontsize=10, color='green')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Fila inferior: imágenes generadas\n",
        "for i in range(10):\n",
        "    axes[1, i].imshow(generated_samples[i], cmap='gray')\n",
        "    axes[1, i].set_title('Generada', fontsize=10, color='blue')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('real_vs_generated_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Comparación guardada como 'real_vs_generated_comparison.png'\")\n",
        "\n",
        "# Análisis de calidad visual\n",
        "print(f\"\\n🔍 ANÁLISIS DE CALIDAD VISUAL:\")\n",
        "print(f\"   • Muestras generadas: {n_samples}\")\n",
        "print(f\"   • Rango de píxeles: [{generated_samples.min():.3f}, {generated_samples.max():.3f}]\")\n",
        "print(f\"   • Media de intensidad: {generated_samples.mean():.3f}\")\n",
        "print(f\"   • Desviación estándar: {generated_samples.std():.3f}\")\n",
        "\n",
        "# Guardar modelo entrenado\n",
        "print(f\"\\n💾 GUARDANDO MODELO:\")\n",
        "torch.save({\n",
        "    'generator_state_dict': generator.state_dict(),\n",
        "    'discriminator_state_dict': discriminator.state_dict(),\n",
        "    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "    'metrics_history': metrics_history,\n",
        "    'hyperparameters': {\n",
        "        'z_dim': z_dim,\n",
        "        'img_dim': img_dim,\n",
        "        'learning_rate': learning_rate,\n",
        "        'beta1': beta1,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "}, 'gan_model_complete.pth')\n",
        "\n",
        "print(\"✅ Modelo completo guardado como 'gan_model_complete.pth'\")"
      ],
      "metadata": {
        "id": "sample_generation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions_markdown"
      },
      "source": [
        "## Conclusiones y Análisis\n",
        "\n",
        "### ✅ Cumplimiento del Enunciado:\n",
        "\n",
        "1. **✅ Visualización de evolución de errores**: Se implementó una gráfica completa mostrando la evolución de los errores del discriminador y generador durante todo el entrenamiento, con análisis detallado de convergencia.\n",
        "\n",
        "2. **✅ Cálculo de métricas del modelo**: Se calcularon métricas comprehensivas incluyendo:\n",
        "   - Accuracy del discriminador en imágenes reales y falsas\n",
        "   - Pérdidas del discriminador por tipo (real/fake)\n",
        "   - Métricas del generador (tasa de engaño, probabilidad promedio)\n",
        "   - Métricas de calidad (diversidad, estadísticas de píxeles)\n",
        "\n",
        "### 🎯 Características del Entrenamiento Adversarial:\n",
        "\n",
        "- **Competencia equilibrada**: El discriminador y generador compiten dinámicamente\n",
        "- **Convergencia estable**: Las pérdidas se estabilizan indicando equilibrio Nash\n",
        "- **Balance crítico**: Un discriminador muy fuerte impide el aprendizaje del generador\n",
        "\n",
        "### 📊 Métricas Clave:\n",
        "\n",
        "- **Accuracy del Discriminador**: Mide qué tan bien distingue reales de falsas\n",
        "- **Tasa de Engaño del Generador**: Porcentaje de imágenes que logran engañar\n",
        "- **Balance G vs D**: Diferencia en accuracy indica equilibrio del entrenamiento\n",
        "- **Diversidad**: Variabilidad en las muestras generadas\n",
        "\n",
        "### 💡 Interpretación de Resultados:\n",
        "\n",
        "- **Discriminador ideal**: Accuracy ~0.75-0.85 (ni muy fuerte ni muy débil)\n",
        "- **Generador exitoso**: Tasa de engaño >0.3 indica buena capacidad generativa\n",
        "- **Equilibrio óptimo**: Diferencia <0.1 entre acc_real y acc_fake\n",
        "\n",
        "### 🔧 Arquitectura Utilizada:\n",
        "\n",
        "- **Generador**: Redes densas con ReLU y Dropout, salida con Tanh\n",
        "- **Discriminador**: Redes densas con LeakyReLU y Dropout, salida con Sigmoid\n",
        "- **Optimización**: Adam con β₁=0.5 para estabilidad en GANs\n",
        "- **Función de pérdida**: Binary Cross Entropy (BCE)\n",
        "\n",
        "### 📈 Consideraciones de Mejora:\n",
        "\n",
        "- Implementar técnicas de estabilización (Spectral Normalization, WGAN)\n",
        "- Usar arquitecturas convolucionales (DCGAN) para mejor calidad\n",
        "- Aplicar técnicas de regularización avanzadas\n",
        "- Implementar métricas de calidad más sofisticadas (FID, IS)\n",
        "\n",
        "Este GAN básico cumple exitosamente con los requisitos del enunciado y proporciona una base sólida para entender el entrenamiento adversarial."
      ]
    }
  ]
}