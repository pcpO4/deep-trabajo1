{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Pr√°ctica 4: Transfer Learning con Modelos Pre-entrenados para MNIST\n",
        "\n",
        "En esta pr√°ctica implementaremos transfer learning utilizando modelos pre-entrenados de TensorFlow para clasificar d√≠gitos MNIST. Compararemos diferentes arquitecturas (VGG16, ResNet50, MobileNetV2) con un modelo CNN b√°sico para evaluar la efectividad del transfer learning.\n",
        "\n",
        "## Objetivos:\n",
        "1. Implementar transfer learning con modelos de tf.keras.applications\n",
        "2. Adaptar modelos pre-entrenados en ImageNet para MNIST\n",
        "3. Comparar rendimiento vs modelo CNN desde cero\n",
        "4. Aplicar fine-tuning para mejorar resultados\n",
        "5. Analizar arquitecturas y caracter√≠sticas de cada modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 1: IMPORTACI√ìN DE LIBRER√çAS Y CONFIGURACI√ìN\n",
        "# ============================================================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n para reproducibilidad\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Verificar GPU\n",
        "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Versi√≥n de TensorFlow: {tf.__version__}\")\n"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 2: CARGA Y PREPROCESAMIENTO DE DATOS\n",
        "# ============================================================================================\n",
        "\n",
        "# Cargar MNIST\n",
        "print(\"Cargando dataset MNIST...\")\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalizar p√≠xeles [0, 255] -> [0, 1]\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Adaptaci√≥n para modelos pre-entrenados:\n",
        "# 1. Convertir a RGB (MNIST es escala de grises)\n",
        "# 2. Redimensionar a 32x32 (m√≠nimo para modelos pre-entrenados)\n",
        "train_images_rgb = np.stack([train_images] * 3, axis=-1)\n",
        "test_images_rgb = np.stack([test_images] * 3, axis=-1)\n",
        "\n",
        "train_images_resized = tf.image.resize(train_images_rgb, [32, 32]).numpy()\n",
        "test_images_resized = tf.image.resize(test_images_rgb, [32, 32]).numpy()\n",
        "\n",
        "# Convertir etiquetas a one-hot encoding\n",
        "train_labels_categorical = to_categorical(train_labels, 10)\n",
        "test_labels_categorical = to_categorical(test_labels, 10)\n",
        "\n",
        "print(f\"\\nDimensiones de los datos:\")\n",
        "print(f\"  Train images: {train_images_resized.shape}\")\n",
        "print(f\"  Test images: {test_images_resized.shape}\")\n",
        "print(f\"  Train labels: {train_labels_categorical.shape}\")\n",
        "print(f\"  Test labels: {test_labels_categorical.shape}\")\n",
        "\n",
        "# Crear conjunto de validaci√≥n\n",
        "validation_split = 0.1\n",
        "print(f\"\\nUsando {validation_split*100}% de datos para validaci√≥n\")"
      ],
      "metadata": {
        "id": "data_preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 3: DEFINICI√ìN DE ARQUITECTURAS DE MODELOS\n",
        "# ============================================================================================\n",
        "\n",
        "def create_transfer_vgg16():\n",
        "    \"\"\"\n",
        "    VGG16 con Transfer Learning\n",
        "    - 16 capas (13 convolucionales + 3 densas)\n",
        "    - Pre-entrenado en ImageNet\n",
        "    - Capas convolucionales congeladas\n",
        "    \"\"\"\n",
        "    base_model = VGG16(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(32, 32, 3)\n",
        "    )\n",
        "    \n",
        "    # Congelar capas base\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', name='dense_256'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax', name='output')\n",
        "    ], name='VGG16_Transfer')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_transfer_resnet50():\n",
        "    \"\"\"\n",
        "    ResNet50 con Transfer Learning\n",
        "    - 50 capas con bloques residuales\n",
        "    - Pre-entrenado en ImageNet\n",
        "    - Incluye BatchNormalization\n",
        "    \"\"\"\n",
        "    base_model = ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(32, 32, 3)\n",
        "    )\n",
        "    \n",
        "    base_model.trainable = False\n",
        "    \n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', name='dense_256'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax', name='output')\n",
        "    ], name='ResNet50_Transfer')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_transfer_mobilenet():\n",
        "    \"\"\"\n",
        "    MobileNetV2 con Transfer Learning\n",
        "    - Arquitectura optimizada para m√≥viles\n",
        "    - Menor n√∫mero de par√°metros\n",
        "    - Separable convolutions\n",
        "    \"\"\"\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(32, 32, 3)\n",
        "    )\n",
        "    \n",
        "    base_model.trainable = False\n",
        "    \n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128, activation='relu', name='dense_128'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(10, activation='softmax', name='output')\n",
        "    ], name='MobileNetV2_Transfer')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_simple_cnn():\n",
        "    \"\"\"\n",
        "    CNN b√°sica como baseline para comparar\n",
        "    - Entrenada desde cero\n",
        "    - Arquitectura simple pero efectiva\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', \n",
        "                      input_shape=(32, 32, 3), padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ], name='CNN_Baseline')\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"Arquitecturas de modelos definidas correctamente\")"
      ],
      "metadata": {
        "id": "model_architectures"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 4: ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS\n",
        "# ============================================================================================\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "PATIENCE = 3  # Para early stopping\n",
        "\n",
        "# Callback para early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=PATIENCE,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "models_dict = {}\n",
        "histories = {}\n",
        "training_times = {}\n",
        "evaluation_results = {}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ENTRENAMIENTO DE MODELOS CON TRANSFER LEARNING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Lista de modelos a entrenar\n",
        "model_configs = [\n",
        "    ('VGG16_Transfer', create_transfer_vgg16, 0.0001),\n",
        "    ('ResNet50_Transfer', create_transfer_resnet50, 0.0001),\n",
        "    ('MobileNetV2_Transfer', create_transfer_mobilenet, 0.0001),\n",
        "    ('CNN_Baseline', create_simple_cnn, 0.001)\n",
        "]\n",
        "\n",
        "import time\n",
        "\n",
        "for model_name, model_func, learning_rate in model_configs:\n",
        "    print(f\"\\n{'-' * 60}\")\n",
        "    print(f\"ENTRENANDO: {model_name}\")\n",
        "    print(f\"{'-' * 60}\")\n",
        "    \n",
        "    # Crear modelo\n",
        "    model = model_func()\n",
        "    models_dict[model_name] = model\n",
        "    \n",
        "    # Compilar con optimizador apropiado\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Mostrar informaci√≥n del modelo\n",
        "    total_params = model.count_params()\n",
        "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "    non_trainable_params = total_params - trainable_params\n",
        "    \n",
        "    print(f\"\\nInformaci√≥n del modelo:\")\n",
        "    print(f\"  Par√°metros totales: {total_params:,}\")\n",
        "    print(f\"  Par√°metros entrenables: {trainable_params:,}\")\n",
        "    print(f\"  Par√°metros congelados: {non_trainable_params:,}\")\n",
        "    print(f\"  % Congelados: {(non_trainable_params/total_params)*100:.1f}%\")\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    start_time = time.time()\n",
        "    \n",
        "    history = model.fit(\n",
        "        train_images_resized,\n",
        "        train_labels_categorical,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    training_times[model_name] = elapsed_time\n",
        "    \n",
        "    print(f\"\\n‚è±Ô∏è  Tiempo de entrenamiento: {elapsed_time/60:.2f} minutos\")\n",
        "    \n",
        "    # Guardar historial\n",
        "    histories[model_name] = history.history\n",
        "    \n",
        "    # Evaluar en conjunto de prueba\n",
        "    test_loss, test_accuracy = model.evaluate(\n",
        "        test_images_resized,\n",
        "        test_labels_categorical,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    evaluation_results[model_name] = {\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_loss': test_loss,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'training_time': elapsed_time\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Resultados - {model_name}:\")\n",
        "    print(f\"   Accuracy: {test_accuracy*100:.2f}%\")\n",
        "    print(f\"   Loss: {test_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "training_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 5: FINE-TUNING DEL MEJOR MODELO\n",
        "# ============================================================================================\n",
        "\n",
        "def create_finetuned_vgg16():\n",
        "    \"\"\"\n",
        "    VGG16 con Fine-tuning\n",
        "    - Descongelar las √∫ltimas 4 capas\n",
        "    - Learning rate muy bajo\n",
        "    \"\"\"\n",
        "    base_model = VGG16(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(32, 32, 3)\n",
        "    )\n",
        "    \n",
        "    # Estrategia de fine-tuning\n",
        "    base_model.trainable = True\n",
        "    \n",
        "    # Congelar todas las capas excepto las √∫ltimas 4\n",
        "    for layer in base_model.layers[:-4]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', name='dense_256'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(10, activation='softmax', name='output')\n",
        "    ], name='VGG16_FineTuned')\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINE-TUNING: VGG16 con capas descongeladas\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_finetuned = create_finetuned_vgg16()\n",
        "\n",
        "# Compilar con learning rate MUY bajo\n",
        "model_finetuned.compile(\n",
        "    optimizer=Adam(learning_rate=0.00001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Mostrar informaci√≥n\n",
        "total_params_ft = model_finetuned.count_params()\n",
        "trainable_params_ft = sum([tf.size(w).numpy() for w in model_finetuned.trainable_weights])\n",
        "non_trainable_params_ft = total_params_ft - trainable_params_ft\n",
        "\n",
        "print(f\"\\nInformaci√≥n Fine-tuning:\")\n",
        "print(f\"  Par√°metros entrenables: {trainable_params_ft:,}\")\n",
        "print(f\"  Par√°metros congelados: {non_trainable_params_ft:,}\")\n",
        "print(f\"  % Entrenables: {(trainable_params_ft/total_params_ft)*100:.1f}%\")\n",
        "\n",
        "# Entrenar con menos √©pocas\n",
        "start_time = time.time()\n",
        "\n",
        "history_finetuned = model_finetuned.fit(\n",
        "    train_images_resized,\n",
        "    train_labels_categorical,\n",
        "    epochs=5,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=validation_split,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\n‚è±Ô∏è  Tiempo de fine-tuning: {elapsed_time/60:.2f} minutos\")\n",
        "\n",
        "# Evaluar modelo fine-tuned\n",
        "test_loss_ft, test_accuracy_ft = model_finetuned.evaluate(\n",
        "    test_images_resized,\n",
        "    test_labels_categorical,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ VGG16_FineTuned:\")\n",
        "print(f\"   Accuracy: {test_accuracy_ft*100:.2f}%\")\n",
        "print(f\"   Loss: {test_loss_ft:.4f}\")\n",
        "\n",
        "# Agregar a resultados\n",
        "histories['VGG16_FineTuned'] = history_finetuned.history\n",
        "evaluation_results['VGG16_FineTuned'] = {\n",
        "    'test_accuracy': test_accuracy_ft,\n",
        "    'test_loss': test_loss_ft,\n",
        "    'total_params': total_params_ft,\n",
        "    'trainable_params': trainable_params_ft,\n",
        "    'training_time': elapsed_time\n",
        "}"
      ],
      "metadata": {
        "id": "finetuning_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 6: COMPARACI√ìN DE RESULTADOS\n",
        "# ============================================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARACI√ìN DE RESULTADOS: TRANSFER LEARNING vs BASELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Crear tabla comparativa\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, results in evaluation_results.items():\n",
        "    comparison_data.append({\n",
        "        'Modelo': model_name,\n",
        "        'Accuracy (%)': f\"{results['test_accuracy']*100:.2f}\",\n",
        "        'Loss': f\"{results['test_loss']:.4f}\",\n",
        "        'Par√°metros Totales': f\"{results['total_params']:,}\",\n",
        "        'Par√°metros Entrenables': f\"{results['trainable_params']:,}\",\n",
        "        'Tiempo (min)': f\"{results['training_time']/60:.2f}\"\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\nTabla Comparativa:\")\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Encontrar el mejor modelo\n",
        "best_model_name = max(evaluation_results.keys(), \n",
        "                      key=lambda x: evaluation_results[x]['test_accuracy'])\n",
        "best_accuracy = evaluation_results[best_model_name]['test_accuracy']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   Accuracy: {best_accuracy*100:.2f}%\")\n",
        "\n",
        "# An√°lisis de eficiencia\n",
        "print(f\"\\nüìà AN√ÅLISIS DE EFICIENCIA:\")\n",
        "for model_name, results in evaluation_results.items():\n",
        "    params_per_accuracy = results['trainable_params'] / (results['test_accuracy'] * 100)\n",
        "    print(f\"  {model_name}: {params_per_accuracy:,.0f} params/accuracy%\")"
      ],
      "metadata": {
        "id": "comparison_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 7: VISUALIZACI√ìN DE RESULTADOS\n",
        "# ============================================================================================\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Gr√°fico de comparaci√≥n - Training curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Transfer Learning vs CNN Baseline - Curvas de Entrenamiento', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "# Training Accuracy\n",
        "ax = axes[0, 0]\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    epochs_range = range(1, len(history['accuracy']) + 1)\n",
        "    ax.plot(epochs_range, history['accuracy'], \n",
        "           label=name, linewidth=2, color=colors[i % len(colors)])\n",
        "ax.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('√âpoca')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Accuracy\n",
        "ax = axes[0, 1]\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    epochs_range = range(1, len(history['val_accuracy']) + 1)\n",
        "    ax.plot(epochs_range, history['val_accuracy'], \n",
        "           label=name, linewidth=2, color=colors[i % len(colors)])\n",
        "ax.set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('√âpoca')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training Loss\n",
        "ax = axes[1, 0]\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    epochs_range = range(1, len(history['loss']) + 1)\n",
        "    ax.plot(epochs_range, history['loss'], \n",
        "           label=name, linewidth=2, color=colors[i % len(colors)])\n",
        "ax.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('√âpoca')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Loss\n",
        "ax = axes[1, 1]\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    epochs_range = range(1, len(history['val_loss']) + 1)\n",
        "    ax.plot(epochs_range, history['val_loss'], \n",
        "           label=name, linewidth=2, color=colors[i % len(colors)])\n",
        "ax.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('√âpoca')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('transfer_learning_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüíæ Gr√°fica guardada como 'transfer_learning_comparison.png'\")\n",
        "\n",
        "# Gr√°fico de barras - Comparaci√≥n de accuracies\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "model_names = list(evaluation_results.keys())\n",
        "accuracies = [results['test_accuracy'] * 100 for results in evaluation_results.values()]\n",
        "\n",
        "bars = plt.bar(model_names, accuracies, color=colors[:len(model_names)], alpha=0.8)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "            f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.title('Comparaci√≥n de Test Accuracy - Transfer Learning vs Baseline', \n",
        "         fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Modelo')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.ylim(0, 100)\n",
        "plt.tight_layout()\n",
        "plt.savefig('accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üíæ Gr√°fica guardada como 'accuracy_comparison.png'\")"
      ],
      "metadata": {
        "id": "visualization_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 8: AN√ÅLISIS DE FEATURE MAPS\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"AN√ÅLISIS DE FEATURE MAPS - VGG16\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Seleccionar una imagen de ejemplo\n",
        "example_idx = 0\n",
        "example_image = test_images_resized[example_idx:example_idx+1]\n",
        "example_label = test_labels[example_idx]\n",
        "\n",
        "print(f\"Analizando imagen del d√≠gito: {example_label}\")\n",
        "\n",
        "# Extraer modelo base VGG16\n",
        "vgg16_model = models_dict['VGG16_Transfer'].layers[0]\n",
        "\n",
        "# Obtener capas convolucionales\n",
        "conv_layers = [layer for layer in vgg16_model.layers if 'conv' in layer.name][:3]\n",
        "layer_names = [layer.name for layer in conv_layers]\n",
        "\n",
        "print(f\"Visualizando feature maps de: {layer_names}\")\n",
        "\n",
        "# Crear modelo para extraer features\n",
        "layer_outputs = [layer.output for layer in conv_layers]\n",
        "activation_model = models.Model(inputs=vgg16_model.input, outputs=layer_outputs)\n",
        "\n",
        "# Obtener activaciones\n",
        "activations = activation_model.predict(example_image, verbose=0)\n",
        "\n",
        "# Visualizar\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle(f'Feature Maps de VGG16 para d√≠gito {example_label}', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Imagen original\n",
        "axes[0, 0].imshow(example_image[0, :, :, 0], cmap='gray')\n",
        "axes[0, 0].set_title('Imagen Original', fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Feature maps\n",
        "for i, (activation, layer_name) in enumerate(zip(activations, layer_names)):\n",
        "    if i < 5:  # Mostrar solo 5 feature maps\n",
        "        row = (i + 1) // 3\n",
        "        col = (i + 1) % 3\n",
        "        \n",
        "        # Tomar primer filtro de la activaci√≥n\n",
        "        feature_map = activation[0, :, :, 0]\n",
        "        \n",
        "        axes[row, col].imshow(feature_map, cmap='viridis')\n",
        "        axes[row, col].set_title(f'{layer_name}\\nShape: {activation.shape[1:]}', \n",
        "                                fontweight='bold')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_maps_vgg16.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üíæ Feature maps guardados como 'feature_maps_vgg16.png'\")"
      ],
      "metadata": {
        "id": "feature_analysis_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 9: PREDICCIONES Y MATRIZ DE CONFUSI√ìN\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUACI√ìN DETALLADA DEL MEJOR MODELO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Usar el mejor modelo para an√°lisis detallado\n",
        "best_model = models_dict[best_model_name]\n",
        "print(f\"Modelo seleccionado: {best_model_name}\")\n",
        "\n",
        "# Predicciones en conjunto de test\n",
        "predictions = best_model.predict(test_images_resized, verbose=0)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Reporte de clasificaci√≥n\n",
        "print(f\"\\nüìä Reporte de Clasificaci√≥n:\")\n",
        "print(classification_report(test_labels, predicted_labels, digits=4))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title(f'Matriz de Confusi√≥n - {best_model_name}', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Etiqueta Real')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üíæ Matriz de confusi√≥n guardada como 'confusion_matrix.png'\")"
      ],
      "metadata": {
        "id": "detailed_evaluation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================================\n",
        "# SECCI√ìN 10: EJEMPLOS DE PREDICCIONES\n",
        "# ============================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"EJEMPLOS DE PREDICCIONES - {best_model_name}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Seleccionar ejemplos aleatorios\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(test_images), 12, replace=False)\n",
        "\n",
        "sample_images = test_images_resized[sample_indices]\n",
        "sample_labels_true = test_labels[sample_indices]\n",
        "\n",
        "# Predecir\n",
        "sample_predictions = best_model.predict(sample_images, verbose=0)\n",
        "sample_labels_pred = np.argmax(sample_predictions, axis=1)\n",
        "\n",
        "# Visualizar\n",
        "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "fig.suptitle(f'Ejemplos de Predicciones - {best_model_name}', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(12):\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "    \n",
        "    # Mostrar imagen original (primer canal)\n",
        "    axes[row, col].imshow(sample_images[i, :, :, 0], cmap='gray')\n",
        "    axes[row, col].axis('off')\n",
        "    \n",
        "    # Color seg√∫n acierto/error\n",
        "    color = 'green' if sample_labels_pred[i] == sample_labels_true[i] else 'red'\n",
        "    confidence = sample_predictions[i][sample_labels_pred[i]] * 100\n",
        "    \n",
        "    title = f\"Pred: {sample_labels_pred[i]}\\nReal: {sample_labels_true[i]}\\n{confidence:.1f}%\"\n",
        "    axes[row, col].set_title(title, color=color, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prediction_examples.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üíæ Ejemplos guardados como 'prediction_examples.png'\")"
      ],
      "metadata": {
        "id": "predictions_examples_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions_markdown"
      },
      "source": [
        "## Conclusiones y An√°lisis\n",
        "\n",
        "### üéØ Ventajas del Transfer Learning:\n",
        "\n",
        "1. **Reutilizaci√≥n de conocimiento**: Los modelos pre-entrenados en ImageNet ya tienen caracter√≠sticas b√°sicas aprendidas (bordes, formas, patrones)\n",
        "\n",
        "2. **Menor tiempo de entrenamiento**: Al congelar las capas base, se reduce significativamente el n√∫mero de par√°metros a entrenar\n",
        "\n",
        "3. **Mejor generalizaci√≥n**: Los features de bajo nivel aprendidos en ImageNet son √∫tiles para MNIST\n",
        "\n",
        "4. **Eficiencia computacional**: Menos par√°metros entrenables = menos recursos computacionales\n",
        "\n",
        "### üìä Comparaci√≥n de Arquitecturas:\n",
        "\n",
        "- **VGG16**: Arquitectura simple y profunda, muchos par√°metros\n",
        "- **ResNet50**: Bloques residuales permiten entrenar redes m√°s profundas\n",
        "- **MobileNetV2**: Optimizada para eficiencia, menos par√°metros\n",
        "\n",
        "### üîß Fine-tuning:\n",
        "\n",
        "El fine-tuning permite ajustar las capas superiores del modelo base para el dominio espec√≠fico, potencialmente mejorando el rendimiento a costa de mayor tiempo de entrenamiento.\n",
        "\n",
        "### üí° Consideraciones:\n",
        "\n",
        "- MNIST es relativamente simple, por lo que la diferencia con CNN b√°sica puede ser menor\n",
        "- En datasets m√°s complejos, el transfer learning mostrar√≠a ventajas m√°s significativas\n",
        "- La elecci√≥n del modelo depende del balance entre precisi√≥n y eficiencia computacional"
      ]
    }
  ]
}