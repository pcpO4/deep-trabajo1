{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Práctica 2: Estudio de Hiperparámetros y Clasificación Par/Impar\n",
        "\n",
        "**Objetivos:**\n",
        "1. Estudiar comportamiento de hiperparámetros de entrenamiento\n",
        "2. Analizar impacto de: neuronas ocultas, épocas, función de pérdida, batch size, learning rate, % validación\n",
        "3. Modificar modelo para clasificación par/impar\n",
        "4. Encontrar configuración óptima"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración inicial\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalización\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten para MLP\n",
        "x_train_flat = x_train.reshape(-1, 28*28)\n",
        "x_test_flat = x_test.reshape(-1, 28*28)\n",
        "\n",
        "# One-hot encoding\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Datos preparados: {x_train_flat.shape}\")"
      ],
      "metadata": {
        "id": "data_preparation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para crear modelos MLP\n",
        "def create_mlp_model(hidden_neurons=128, learning_rate=0.001, loss='categorical_crossentropy', output_classes=10):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden_neurons, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(output_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"Función de creación de modelos definida\")"
      ],
      "metadata": {
        "id": "model_definition"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 1: Número de neuronas ocultas\n",
        "print(\"=== ESTUDIO 1: NEURONAS OCULTAS ===\")\n",
        "\n",
        "hidden_neurons_values = [32, 64, 128, 256, 512]\n",
        "neuron_results = {}\n",
        "\n",
        "for neurons in hidden_neurons_values:\n",
        "    print(f\"Probando {neurons} neuronas...\")\n",
        "    \n",
        "    model = create_mlp_model(hidden_neurons=neurons)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_cat,\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "    \n",
        "    neuron_results[neurons] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time,\n",
        "        'params': model.count_params()\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc:.4f}, Params: {model.count_params():,}\")\n",
        "\n",
        "print(\"Estudio de neuronas completado\")"
      ],
      "metadata": {
        "id": "neurons_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 2: Número de épocas\n",
        "print(\"\\n=== ESTUDIO 2: NÚMERO DE ÉPOCAS ===\")\n",
        "\n",
        "epochs_values = [5, 10, 15, 20, 30]\n",
        "epochs_results = {}\n",
        "\n",
        "for epochs in epochs_values:\n",
        "    print(f\"Probando {epochs} épocas...\")\n",
        "    \n",
        "    model = create_mlp_model()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_cat,\n",
        "        epochs=epochs,\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "    \n",
        "    epochs_results[epochs] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc:.4f}, Tiempo: {training_time:.2f}s\")\n",
        "\n",
        "print(\"Estudio de épocas completado\")"
      ],
      "metadata": {
        "id": "epochs_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 3: Batch size\n",
        "print(\"\\n=== ESTUDIO 3: BATCH SIZE ===\")\n",
        "\n",
        "batch_sizes = [32, 64, 128, 256, 512]\n",
        "batch_results = {}\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"Probando batch size {batch_size}...\")\n",
        "    \n",
        "    model = create_mlp_model()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_cat,\n",
        "        epochs=10,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "    \n",
        "    batch_results[batch_size] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc:.4f}, Tiempo: {training_time:.2f}s\")\n",
        "\n",
        "print(\"Estudio de batch size completado\")"
      ],
      "metadata": {
        "id": "batch_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 4: Learning rate\n",
        "print(\"\\n=== ESTUDIO 4: LEARNING RATE ===\")\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "lr_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Probando learning rate {lr}...\")\n",
        "    \n",
        "    model = create_mlp_model(learning_rate=lr)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_cat,\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "    \n",
        "    lr_results[lr] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"Estudio de learning rate completado\")"
      ],
      "metadata": {
        "id": "lr_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 5: Porcentaje de validación\n",
        "print(\"\\n=== ESTUDIO 5: % VALIDACIÓN ===\")\n",
        "\n",
        "validation_splits = [0.05, 0.1, 0.15, 0.2, 0.3]\n",
        "val_results = {}\n",
        "\n",
        "for val_split in validation_splits:\n",
        "    print(f\"Probando validación {val_split*100:.0f}%...\")\n",
        "    \n",
        "    model = create_mlp_model()\n",
        "    \n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_cat,\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        validation_split=val_split,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "    \n",
        "    val_results[val_split] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'val_accuracy': history.history['val_accuracy'][-1]\n",
        "    }\n",
        "    \n",
        "    print(f\"  Test Acc: {test_acc:.4f}, Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "print(\"Estudio de % validación completado\")"
      ],
      "metadata": {
        "id": "validation_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTUDIO 6: Funciones de pérdida\n",
        "print(\"\\n=== ESTUDIO 6: FUNCIONES DE PÉRDIDA ===\")\n",
        "\n",
        "loss_functions = {\n",
        "    'categorical_crossentropy': 'categorical_crossentropy',\n",
        "    'sparse_categorical_crossentropy': 'sparse_categorical_crossentropy'\n",
        "}\n",
        "\n",
        "loss_results = {}\n",
        "\n",
        "for loss_name, loss_func in loss_functions.items():\n",
        "    print(f\"Probando {loss_name}...\")\n",
        "    \n",
        "    model = create_mlp_model(loss=loss_func)\n",
        "    \n",
        "    # Usar labels apropiadas\n",
        "    if loss_func == 'sparse_categorical_crossentropy':\n",
        "        y_train_use = y_train\n",
        "        y_test_use = y_test\n",
        "    else:\n",
        "        y_train_use = y_train_cat\n",
        "        y_test_use = y_test_cat\n",
        "    \n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_use,\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_use, verbose=0)\n",
        "    \n",
        "    loss_results[loss_name] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"Estudio de funciones de pérdida completado\")"
      ],
      "metadata": {
        "id": "loss_study"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de estudios\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Estudio de Hiperparámetros - MNIST', fontsize=16)\n",
        "\n",
        "# 1. Neuronas\n",
        "ax = axes[0, 0]\n",
        "neurons = list(neuron_results.keys())\n",
        "accs = [neuron_results[n]['test_accuracy'] for n in neurons]\n",
        "ax.plot(neurons, accs, 'bo-', linewidth=2)\n",
        "ax.set_xlabel('Neuronas Ocultas')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Neuronas vs Accuracy')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Épocas\n",
        "ax = axes[0, 1]\n",
        "epochs = list(epochs_results.keys())\n",
        "accs = [epochs_results[e]['test_accuracy'] for e in epochs]\n",
        "ax.plot(epochs, accs, 'ro-', linewidth=2)\n",
        "ax.set_xlabel('Épocas')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Épocas vs Accuracy')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Batch size\n",
        "ax = axes[0, 2]\n",
        "batches = list(batch_results.keys())\n",
        "accs = [batch_results[b]['test_accuracy'] for b in batches]\n",
        "ax.plot(batches, accs, 'go-', linewidth=2)\n",
        "ax.set_xlabel('Batch Size')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Batch Size vs Accuracy')\n",
        "ax.set_xscale('log', base=2)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Learning rate\n",
        "ax = axes[1, 0]\n",
        "lrs = list(lr_results.keys())\n",
        "accs = [lr_results[lr]['test_accuracy'] for lr in lrs]\n",
        "ax.semilogx(lrs, accs, 'mo-', linewidth=2)\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Learning Rate vs Accuracy')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Validación\n",
        "ax = axes[1, 1]\n",
        "vals = [v*100 for v in val_results.keys()]\n",
        "test_accs = [val_results[v]['test_accuracy'] for v in val_results.keys()]\n",
        "val_accs = [val_results[v]['val_accuracy'] for v in val_results.keys()]\n",
        "ax.plot(vals, test_accs, 'co-', linewidth=2, label='Test')\n",
        "ax.plot(vals, val_accs, 'yo-', linewidth=2, label='Validación')\n",
        "ax.set_xlabel('% Validación')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('% Validación vs Accuracy')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Loss functions\n",
        "ax = axes[1, 2]\n",
        "loss_names = list(loss_results.keys())\n",
        "accs = [loss_results[l]['test_accuracy'] for l in loss_names]\n",
        "bars = ax.bar(range(len(loss_names)), accs, color=['skyblue', 'lightcoral'], alpha=0.8)\n",
        "ax.set_xlabel('Función de Pérdida')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Loss Function vs Accuracy')\n",
        "ax.set_xticks(range(len(loss_names)))\n",
        "ax.set_xticklabels(['Categorical', 'Sparse'], rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hyperparams_visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASIFICACIÓN PAR/IMPAR\n",
        "print(\"\\n=== CLASIFICACIÓN PAR/IMPAR ===\")\n",
        "\n",
        "# Crear labels par/impar\n",
        "y_train_par_impar = (y_train % 2).astype('int32')  # 0=par, 1=impar\n",
        "y_test_par_impar = (y_test % 2).astype('int32')\n",
        "y_train_par_impar_cat = tf.keras.utils.to_categorical(y_train_par_impar, 2)\n",
        "y_test_par_impar_cat = tf.keras.utils.to_categorical(y_test_par_impar, 2)\n",
        "\n",
        "print(f\"Labels transformadas: 0=par, 1=impar\")\n",
        "print(f\"Distribución: Pares={np.sum(y_train_par_impar==0)}, Impares={np.sum(y_train_par_impar==1)}\")\n",
        "\n",
        "# Modelo para clasificación par/impar\n",
        "def create_par_impar_model(hidden_neurons=128, learning_rate=0.001, dropout_rate=0.3):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden_neurons, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate/2),\n",
        "        tf.keras.layers.Dense(2, activation='softmax')  # 2 clases\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"Modelo par/impar definido\")"
      ],
      "metadata": {
        "id": "parity_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar múltiples configuraciones para par/impar\n",
        "par_impar_configs = [\n",
        "    {'neurons': 256, 'lr': 0.001, 'dropout': 0.2, 'epochs': 15},\n",
        "    {'neurons': 512, 'lr': 0.0005, 'dropout': 0.3, 'epochs': 20},\n",
        "    {'neurons': 128, 'lr': 0.002, 'dropout': 0.25, 'epochs': 12},\n",
        "    {'neurons': 384, 'lr': 0.0008, 'dropout': 0.35, 'epochs': 18}\n",
        "]\n",
        "\n",
        "par_impar_results = {}\n",
        "\n",
        "print(\"Entrenando configuraciones para par/impar:\")\n",
        "\n",
        "for i, config in enumerate(par_impar_configs):\n",
        "    config_name = f\"ParImpar_{i+1}\"\n",
        "    print(f\"\\n{config_name}: {config}\")\n",
        "    \n",
        "    model = create_par_impar_model(\n",
        "        hidden_neurons=config['neurons'],\n",
        "        learning_rate=config['lr'],\n",
        "        dropout_rate=config['dropout']\n",
        "    )\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "    ]\n",
        "    \n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_flat, y_train_par_impar_cat,\n",
        "        epochs=config['epochs'],\n",
        "        batch_size=128,\n",
        "        validation_split=0.15,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(x_test_flat, y_test_par_impar_cat, verbose=0)\n",
        "    \n",
        "    par_impar_results[config_name] = {\n",
        "        'config': config,\n",
        "        'model': model,\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time,\n",
        "        'history': history.history\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {test_acc*100:.3f}%, Tiempo: {training_time:.2f}s\")\n",
        "\n",
        "# Mejor modelo par/impar\n",
        "best_config_name = max(par_impar_results.keys(), key=lambda x: par_impar_results[x]['test_accuracy'])\n",
        "best_par_impar_model = par_impar_results[best_config_name]['model']\n",
        "best_par_impar_acc = par_impar_results[best_config_name]['test_accuracy']\n",
        "\n",
        "print(f\"\\nMejor modelo par/impar: {best_config_name}\")\n",
        "print(f\"Accuracy: {best_par_impar_acc*100:.3f}%\")"
      ],
      "metadata": {
        "id": "parity_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis detallado par/impar\n",
        "y_pred_par_impar = best_par_impar_model.predict(x_test_flat, verbose=0)\n",
        "y_pred_par_impar_classes = np.argmax(y_pred_par_impar, axis=1)\n",
        "\n",
        "# Matriz de confusión\n",
        "cm_par_impar = confusion_matrix(y_test_par_impar, y_pred_par_impar_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_par_impar, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['PAR', 'IMPAR'], yticklabels=['PAR', 'IMPAR'])\n",
        "plt.title(f'Matriz de Confusión - Par/Impar\\nAccuracy: {best_par_impar_acc*100:.2f}%')\n",
        "plt.ylabel('Clase Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nReporte de clasificación par/impar:\")\n",
        "print(classification_report(y_test_par_impar, y_pred_par_impar_classes, \n",
        "                           target_names=['PAR', 'IMPAR']))\n",
        "\n",
        "# Análisis por dígito\n",
        "print(\"\\nAnálisis por dígito original:\")\n",
        "for digit in range(10):\n",
        "    mask = (y_test == digit)\n",
        "    digit_preds = y_pred_par_impar_classes[mask]\n",
        "    digit_true = y_test_par_impar[mask]\n",
        "    accuracy = (digit_preds == digit_true).mean()\n",
        "    expected = 'PAR' if digit % 2 == 0 else 'IMPAR'\n",
        "    print(f\"  Dígito {digit} ({expected}): {accuracy*100:.2f}% accuracy\")"
      ],
      "metadata": {
        "id": "parity_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabla comparativa final\n",
        "print(\"\\nTABLA COMPARATIVA FINAL:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Mejor configuración para cada hiperparámetro\n",
        "best_neurons = max(neuron_results.keys(), key=lambda x: neuron_results[x]['test_accuracy'])\n",
        "best_epochs = max(epochs_results.keys(), key=lambda x: epochs_results[x]['test_accuracy'])\n",
        "best_batch = max(batch_results.keys(), key=lambda x: batch_results[x]['test_accuracy'])\n",
        "best_lr = max(lr_results.keys(), key=lambda x: lr_results[x]['test_accuracy'])\n",
        "best_val = max(val_results.keys(), key=lambda x: val_results[x]['test_accuracy'])\n",
        "\n",
        "# Crear modelo final con mejores hiperparámetros\n",
        "model_final_10 = create_mlp_model(\n",
        "    hidden_neurons=best_neurons,\n",
        "    learning_rate=best_lr\n",
        ")\n",
        "\n",
        "history_final = model_final_10.fit(\n",
        "    x_train_flat, y_train_cat,\n",
        "    epochs=best_epochs,\n",
        "    batch_size=best_batch,\n",
        "    validation_split=best_val,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "test_loss_final_10, test_acc_final_10 = model_final_10.evaluate(x_test_flat, y_test_cat, verbose=0)\n",
        "\n",
        "print(f\"Modelo optimizado 10-dígitos: {test_acc_final_10*100:.3f}%\")\n",
        "print(f\"Modelo optimizado par/impar: {best_par_impar_acc*100:.3f}%\")\n",
        "\n",
        "improvement = (best_par_impar_acc - test_acc_final_10) * 100\n",
        "print(f\"\\nDiferencia: {improvement:+.2f} puntos porcentuales\")"
      ],
      "metadata": {
        "id": "final_comparison"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}